{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e668adaa",
      "metadata": {
        "id": "e668adaa"
      },
      "outputs": [],
      "source": [
        "def is_colab():\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "IS_COLAB = is_colab()\n",
        "\n",
        "if IS_COLAB:\n",
        "  !pip install git+https://github.com/hoggl-dsp/cc_beats\n",
        "\n",
        "  !apt-get install fluidsynth\n",
        "  !mkdir data\n",
        "  !mkdir data/sf2\n",
        "  !wget -O data/sf2/Xpand_2_-_Practice_Room_Kit.sf2 'https://musical-artifacts.com/artifacts/6296/Xpand_2_-_Practice_Room_Kit.sf2'\n",
        "\n",
        "  !wget -O data/groove_midi_only.zip 'https://storage.googleapis.com/magentadata/datasets/groove/groove-v1.0.0-midionly.zip'\n",
        "  !unzip data/groove_midi_only.zip -d data/\n",
        "\n",
        "  !wget -O data/clean_midi.tar.gz 'http://hog.ee.columbia.edu/craffel/lmd/clean_midi.tar.gz'\n",
        "  !tar -xf data/clean_midi.tar.gz -C data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb8fbf63",
      "metadata": {
        "id": "cb8fbf63"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import torch.utils.tensorboard\n",
        "\n",
        "import symusic\n",
        "import symusic.types\n",
        "from midi2audio import FluidSynth\n",
        "\n",
        "import IPython.display as ipd\n",
        "import ipywidgets as widgets\n",
        "import tqdm\n",
        "import os\n",
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "from typing import Callable\n",
        "\n",
        "from cc_beats import tokeniser, modules, utils\n",
        "\n",
        "sf_path = 'data/sf2/Xpand_2_-_Practice_Room_Kit.sf2'\n",
        "FS = FluidSynth(sf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5e76589",
      "metadata": {
        "id": "b5e76589"
      },
      "outputs": [],
      "source": [
        "midi_files = []\n",
        "for root, _, files in os.walk(os.path.join('data', 'groove')):\n",
        "    midi_files.extend([os.path.join(root, file) for file in files if file.endswith('.mid') and 'beat' in file and 'eval' not in os.path.basename(root)])\n",
        "\n",
        "midi_files = []\n",
        "for root, _, files in os.walk(os.path.join('data', 'clean_midi')):\n",
        "    midi_files.extend([os.path.join(root, file) for file in files if file.endswith('.mid')]) # and 'beat' in file and 'eval' not in os.path.basename(root)])\n",
        "\n",
        "for i in range(20):\n",
        "    print(midi_files[i])\n",
        "\n",
        "midi_file = symusic.Score(midi_files[0])\n",
        "\n",
        "encoder = tokeniser.DrumSequenceEncoder(subdivision=16)\n",
        "\n",
        "tok_sequence = encoder.encode(midi_file)\n",
        "returned_midi = encoder.decode(tok_sequence)\n",
        "\n",
        "returned_midi.time_signatures.extend(midi_file.time_signatures)\n",
        "returned_midi.key_signatures.extend(midi_file.key_signatures)\n",
        "returned_midi.tempos.extend(midi_file.tempos)\n",
        "\n",
        "print(\"Actual File\")\n",
        "print(midi_file)\n",
        "print(midi_file.tracks[0].notes[:10])\n",
        "print('-----------------------------')\n",
        "\n",
        "print(\"Decoded File\")\n",
        "print(returned_midi)\n",
        "print(returned_midi.tracks[0].notes[:10])\n",
        "print('-----------------------------')\n",
        "\n",
        "display(utils.midi_to_audio_display(FS, midi_file))\n",
        "display(utils.midi_to_audio_display(FS, returned_midi))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e020893b",
      "metadata": {
        "id": "e020893b"
      },
      "outputs": [],
      "source": [
        "def get_midi_data_files(root_dir: str, filter_fn: Callable[[str, str], bool] = lambda root, file: True):\n",
        "    midi_files = []\n",
        "    for root, _, files in os.walk(root_dir):\n",
        "        midi_files.extend([os.path.join(root, file) for file in files if filter_fn(root, file)])\n",
        "    return midi_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0ebb235",
      "metadata": {
        "id": "c0ebb235"
      },
      "outputs": [],
      "source": [
        "# Lakh clean_set analysis\n",
        "import pandas as pd\n",
        "\n",
        "def do_lakh_analysis():\n",
        "    files = get_midi_data_files(os.path.join('data', 'clean_midi'))\n",
        "\n",
        "    data = {\n",
        "        'File': [],\n",
        "        'File Length (quarters)': [],\n",
        "        'Tempos': [],\n",
        "        'Is 4/4': [],\n",
        "        'Num Drum Tracks': [],\n",
        "        'Drum Hit Count': [],\n",
        "        'Unique Drum Hits': [],\n",
        "        'First Drum (quarters)': [],\n",
        "        'Last Drum (quarters)': [],\n",
        "    }\n",
        "    for file in tqdm.tqdm(files, desc='Analysing Midi Files'):\n",
        "        midi = None\n",
        "        drum_tracks = None\n",
        "        data['File'].append(file)\n",
        "        try:\n",
        "            midi = symusic.Score.from_file(file)\n",
        "            data['File Length (quarters)'].append((midi.end() - midi.start()) / midi.tpq)\n",
        "            data['Tempos'].append(midi.tempos)\n",
        "            data['Is 4/4'].append(len(midi.time_signatures) == 1 and (midi.time_signatures[0].numerator == 4 and midi.time_signatures[0].denominator == 4))\n",
        "\n",
        "            drum_tracks = [track for track in midi.tracks if track.is_drum]\n",
        "            data['Num Drum Tracks'].append(len(drum_tracks))\n",
        "\n",
        "            count = 0\n",
        "            note_set = set()\n",
        "            first_note_time = None\n",
        "            last_note_time = None\n",
        "            for track in drum_tracks:\n",
        "                for note in track.notes:\n",
        "                    if note.pitch not in tokeniser.ROLAND_DRUM_MAPPING:\n",
        "                        continue\n",
        "                    count += 1\n",
        "                    note_set.add(note.pitch)\n",
        "                    if first_note_time is None or first_note_time > note.time:\n",
        "                        first_note_time = note.time\n",
        "                    if last_note_time is None or last_note_time < note.time:\n",
        "                        last_note_time = note.time\n",
        "\n",
        "            data['Drum Hit Count'].append(count)\n",
        "            data['Unique Drum Hits'].append(note_set)\n",
        "            data['First Drum (quarters)'].append(first_note_time / midi.tpq if first_note_time is not None else None)\n",
        "            data['Last Drum (quarters)'].append(last_note_time / midi.tpq if last_note_time is not None else None)\n",
        "\n",
        "        except:\n",
        "            for key in data:\n",
        "                if key != 'File':\n",
        "                    data[key].append(None)\n",
        "\n",
        "\n",
        "\n",
        "    lakh_df = pd.DataFrame(data)\n",
        "\n",
        "    print(\" ***** Filtering Dataset *****\")\n",
        "    print(\"Full Dataset:\", len(lakh_df))\n",
        "    lakh_df = lakh_df[~lakh_df.isna()]\n",
        "    print(\"Without unreadable files:\", len(lakh_df))\n",
        "    lakh_df = lakh_df[lakh_df['Num Drum Tracks'] > 0]\n",
        "    print(\"Without files with no drums:\", len(lakh_df))\n",
        "    lakh_df = lakh_df[lakh_df['Is 4/4']]\n",
        "    print(\"Without files not in 4/4:\", len(lakh_df))\n",
        "    lakh_df = lakh_df[lakh_df['Drum Hit Count'] > 256]\n",
        "    print(\"Without files with less than 256 drum hits\", len(lakh_df))\n",
        "    lakh_df = lakh_df[lakh_df['Drum Hit Count'] / (lakh_df['File Length (quarters)']) >= 1.0]\n",
        "    print(\"Without files with note density < 1 per quarter:\", len(lakh_df))\n",
        "    lakh_df = lakh_df[lakh_df['File Length (quarters)'] < 1000.0]\n",
        "    print(\"Without files with > 1000 beats:\", len(lakh_df))\n",
        "    lakh_df = lakh_df[lakh_df['Unique Drum Hits'].apply(lambda x: len(x) > 2)]\n",
        "    print(\"Without files with no drum hit variety:\", len(lakh_df))\n",
        "\n",
        "    return lakh_df\n",
        "\n",
        "lakh_clean_df = do_lakh_analysis()\n",
        "\n",
        "print(lakh_clean_df.describe())\n",
        "\n",
        "print(lakh_clean_df.head(30))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b1676ca",
      "metadata": {
        "id": "1b1676ca"
      },
      "outputs": [],
      "source": [
        "class DrumInpaintingTransformer(torch.nn.Module):\n",
        "    def __init__(self, num_pitches: int, embedding_dim: int = 64, num_layers: int = 6, num_heads: int = 8, dropout: float = 0.1, pitch_pos_weight: float = 2.0, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        self.input_proj = torch.nn.Linear(1 + num_pitches, embedding_dim)\n",
        "\n",
        "        self.transformer = torch.nn.ModuleList([\n",
        "            modules.TransformerLayerWithRelativeAttention(embedding_dim, num_heads=num_heads, dropout=dropout, max_distance=64)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.output_hit_proj = torch.nn.Linear(embedding_dim, 1)\n",
        "        self.output_pitches_proj = torch.nn.Linear(embedding_dim, num_pitches)\n",
        "        self.output_velocities_proj = torch.nn.Linear(embedding_dim, num_pitches)\n",
        "\n",
        "        if isinstance(pitch_pos_weight, float):\n",
        "            pitch_pos_weight = torch.ones((1, 1, 9)) * pitch_pos_weight\n",
        "        self.pitch_pos_weight = pitch_pos_weight\n",
        "\n",
        "    def forward(self, x: torch.Tensor, attention_mask=None):\n",
        "        # Input dims [batch, seq_len, 1 + 2 * num_pitches]\n",
        "\n",
        "        x = self.input_proj(x) # [batch, seq_len, embedding_dim]\n",
        "\n",
        "        for layer in self.transformer:\n",
        "            x = layer(x) # [batch, seq_len, embedding_dim]\n",
        "\n",
        "        hit_logits = self.output_hit_proj(x)\n",
        "        pitch_logits = self.output_pitches_proj(x)\n",
        "        vel_logits = torch.sigmoid(self.output_velocities_proj(x))\n",
        "\n",
        "        return hit_logits, pitch_logits, vel_logits\n",
        "\n",
        "    def loss_function(self,\n",
        "            preds: tuple[torch.Tensor, torch.Tensor, torch.Tensor],\n",
        "            truths: tuple[torch.Tensor, torch.Tensor, torch.Tensor],\n",
        "            inpainting_mask: torch.Tensor,\n",
        "            unmasked_weight: float,\n",
        "        ) -> torch.Tensor:\n",
        "        pred_hits, pred_pitches, pred_vels = preds\n",
        "        true_hits, true_pitches, true_vels = truths\n",
        "\n",
        "        device = pred_hits.device\n",
        "\n",
        "        hit_weights = torch.ones_like(true_hits)\n",
        "        hit_weights[~inpainting_mask] = unmasked_weight\n",
        "\n",
        "        hit_loss = torch.nn.functional.binary_cross_entropy_with_logits(\n",
        "            pred_hits,\n",
        "            true_hits,\n",
        "            weight=hit_weights\n",
        "        )\n",
        "\n",
        "        hit_mask = inpainting_mask & (true_hits > 0.5)\n",
        "        hit_mask = hit_mask.expand_as(true_pitches)\n",
        "\n",
        "        pitch_weights = torch.ones_like(true_pitches)\n",
        "        pitch_weights[~hit_mask] = unmasked_weight\n",
        "\n",
        "        pitch_loss = torch.nn.functional.binary_cross_entropy_with_logits(\n",
        "            pred_pitches,\n",
        "            true_pitches,\n",
        "            weight=pitch_weights,\n",
        "            pos_weight=self.pitch_pos_weight.to(pred_pitches.device)\n",
        "        )\n",
        "\n",
        "        hit_pitch_mask = hit_mask & (true_pitches > 0.5)\n",
        "        vel_loss = torch.nn.functional.mse_loss(\n",
        "            pred_vels[hit_pitch_mask],\n",
        "            true_vels[hit_pitch_mask]\n",
        "        ) if hit_pitch_mask.any() else torch.tensor(0.0, device=device)\n",
        "\n",
        "        return hit_loss + pitch_loss + vel_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "869a25ec",
      "metadata": {
        "id": "869a25ec"
      },
      "outputs": [],
      "source": [
        "# Dataset\n",
        "def make_dataset(encoder: tokeniser.DrumSequenceEncoder, midi_files: list[str], max_seq_length: int = 64):\n",
        "    sequences = encoder.encode_all(midi_files)\n",
        "    split_hits = []\n",
        "    split_pitches = []\n",
        "    split_velocities = []\n",
        "    for hits, pitches, velocities in sequences:\n",
        "        len_padding = max_seq_length - (hits.size(0) % max_seq_length)\n",
        "        if len_padding > 0:\n",
        "            hits = torch.cat([hits, torch.zeros(len_padding, hits.size(1), dtype=hits.dtype, device=hits.device)])\n",
        "            pitches = torch.cat([pitches, torch.zeros(len_padding, pitches.size(1), dtype=pitches.dtype, device=pitches.device)])\n",
        "            velocities = torch.cat([velocities, torch.zeros(len_padding, velocities.size(1), dtype=velocities.dtype, device=velocities.device)])\n",
        "\n",
        "        split_hits.extend(torch.tensor_split(hits, int(hits.size(0) / max_seq_length)))\n",
        "        split_pitches.extend(torch.tensor_split(pitches, int(pitches.size(0) / max_seq_length)))\n",
        "        split_velocities.extend(torch.tensor_split(velocities, int(velocities.size(0) / max_seq_length)))\n",
        "\n",
        "    return torch.utils.data.TensorDataset(\n",
        "        torch.stack(split_hits, dim=0),\n",
        "        torch.stack(split_pitches, dim=0),\n",
        "        torch.stack(split_velocities, dim=0)\n",
        "    )\n",
        "\n",
        "drum_encoder = tokeniser.DrumSequenceEncoder(subdivision=16)\n",
        "dataset = make_dataset(drum_encoder, lakh_clean_df['File'].to_list(), max_seq_length=128)\n",
        "\n",
        "print(\"Dataset_Size:\", len(dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dad24ff0",
      "metadata": {},
      "outputs": [],
      "source": [
        "if IS_COLAB:\n",
        "    %load_ext tensorboard\n",
        "    %tensorboard --logdir logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd22dc92",
      "metadata": {
        "id": "dd22dc92"
      },
      "outputs": [],
      "source": [
        "# Set up training parameters\n",
        "batch_size = 32\n",
        "\n",
        "mask_prob_config = {\n",
        "    'mean': 0.4,\n",
        "    'std': 0.1,\n",
        "    'min': 0.1,\n",
        "    'max': 0.7,\n",
        "}\n",
        "\n",
        "unmasked_weight_config = {\n",
        "    'initial': 0.5,\n",
        "    'final': 0.1,\n",
        "}\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else \"cpu\")\n",
        "\n",
        "model = DrumInpaintingTransformer(num_pitches=9, embedding_dim=32, num_layers=20, num_heads=16, pitch_pos_weight=3.0)\n",
        "model = model.to(device)  # Move model to the appropriate device\n",
        "\n",
        "# Create a DataLoader for batching\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_set, valid_set = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_subset = torch.utils.data.Subset(train_set, random.sample(range(len(train_set)), 1000))\n",
        "valid_subset = torch.utils.data.Subset(valid_set, random.sample(range(len(valid_set)), 200))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_subset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    valid_subset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "NUM_HYPERPARAM_SEARCHES = 5\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "hyperparam_choices = {\n",
        "    'num_layers': [16, 24, 32],\n",
        "    'num_heads': [8, 16, 32],\n",
        "    'embedding_dim': [16, 32, 64],\n",
        "    'pitch_pos_weight': [1.0, 2.0, 3.0],\n",
        "    'dropout': [0.1, 0.2, 0.3],\n",
        "}\n",
        "\n",
        "\n",
        "def make_example_prompt(length: int, prompt_hits: list[tuple[int, int, float]]):\n",
        "    hits = torch.zeros(1, length, 1)\n",
        "    pitches = torch.zeros(1, length, 9)\n",
        "    velocities = torch.zeros(1, length, 9)\n",
        "\n",
        "    for index, pitch, vel in prompt_hits:\n",
        "        hits[:, index, :] = 1.0\n",
        "        pitches[:, index, pitch] = 1.0\n",
        "        velocities[:, index, pitch] = vel\n",
        "\n",
        "    return hits.to(device), pitches.to(device), velocities.to(device)\n",
        "\n",
        "BACKBEAT = make_example_prompt(16, [\n",
        "    (0, drum_encoder.pitch_to_index[36], 0.8),\n",
        "    (0, drum_encoder.pitch_to_index[46], 0.8),\n",
        "    (2, drum_encoder.pitch_to_index[42], 0.5),\n",
        "    (4, drum_encoder.pitch_to_index[38], 0.8),\n",
        "    (4, drum_encoder.pitch_to_index[42], 0.6),\n",
        "    (6, drum_encoder.pitch_to_index[42], 0.5),\n",
        "    (8, drum_encoder.pitch_to_index[36], 0.8),\n",
        "    (8, drum_encoder.pitch_to_index[42], 0.6),\n",
        "    (10, drum_encoder.pitch_to_index[42], 0.5),\n",
        "    (12, drum_encoder.pitch_to_index[38], 0.8),\n",
        "    (12, drum_encoder.pitch_to_index[42], 0.6),\n",
        "    (14, drum_encoder.pitch_to_index[42], 0.5)\n",
        "])\n",
        "\n",
        "PLEASE = make_example_prompt(32, [\n",
        "    (0, drum_encoder.pitch_to_index[36], 0.8),\n",
        "    (3, drum_encoder.pitch_to_index[36], 0.4),\n",
        "    (4, drum_encoder.pitch_to_index[38], 0.8),\n",
        "    (6, drum_encoder.pitch_to_index[36], 0.7),\n",
        "    (11, drum_encoder.pitch_to_index[36], 0.5),\n",
        "    (12, drum_encoder.pitch_to_index[38], 0.8),\n",
        "    (14, drum_encoder.pitch_to_index[36], 0.7),\n",
        "    (19, drum_encoder.pitch_to_index[36], 0.5),\n",
        "    (20, drum_encoder.pitch_to_index[38], 0.8),\n",
        "    (22, drum_encoder.pitch_to_index[36], 0.7),\n",
        "    (27, drum_encoder.pitch_to_index[36], 0.5),\n",
        "    (28, drum_encoder.pitch_to_index[38], 0.8),\n",
        "    (29, drum_encoder.pitch_to_index[38], 0.3),\n",
        "])\n",
        "\n",
        "AMEN = make_example_prompt(16, [\n",
        "    (0, drum_encoder.pitch_to_index[36], 0.8),\n",
        "    (2, drum_encoder.pitch_to_index[36], 0.6),\n",
        "    (4, drum_encoder.pitch_to_index[38], 0.8),\n",
        "    (7, drum_encoder.pitch_to_index[38], 0.5),\n",
        "    (9, drum_encoder.pitch_to_index[38], 0.4),\n",
        "    (10, drum_encoder.pitch_to_index[36], 0.6),\n",
        "    (11, drum_encoder.pitch_to_index[36], 0.5),\n",
        "    (12, drum_encoder.pitch_to_index[38], 0.8),\n",
        "    (15, drum_encoder.pitch_to_index[38], 0.5)\n",
        "])\n",
        "\n",
        "for _ in range(NUM_HYPERPARAM_SEARCHES):\n",
        "    hyperparams = {key: random.choice(value) for key, value in hyperparam_choices.items()}\n",
        "\n",
        "    print(\"******************** Starting search with hyperparameters: ********************\")\n",
        "    for key, value in hyperparams.items():\n",
        "        print(f\"{key}: {value}\")\n",
        "    print(\"*******************************************************************************\")\n",
        "    print()\n",
        "    model = DrumInpaintingTransformer(num_pitches=9, **hyperparams)\n",
        "    model = model.to(device) \n",
        "\n",
        "    optimiser = torch.optim.Adam(model.parameters(), lr=2e-5, weight_decay=1e-6)\n",
        "\n",
        "    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimiser,\n",
        "        T_max=(NUM_EPOCHS * len(train_loader)) // 10,\n",
        "        eta_min=1e-7\n",
        "    )\n",
        "\n",
        "    log_dir = os.path.join('logs', '-'.join([f'{key}_{value}' for key, value in hyperparams.items()]))\n",
        "\n",
        "    if os.path.exists(log_dir):\n",
        "        i = 0\n",
        "        while os.path.exists(f'{log_dir}_{i}'):\n",
        "            i += 1\n",
        "        log_dir = f'{log_dir}_{i}'\n",
        "    os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "    writer = torch.utils.tensorboard.SummaryWriter(log_dir=log_dir, flush_secs=30)\n",
        "\n",
        "    writer.add_text('hyperparams', str(hyperparams))\n",
        "\n",
        "    for i in range(NUM_EPOCHS):\n",
        "        print(f\"******************** Epoch {i+1}/{NUM_EPOCHS} ********************\")\n",
        "        print(\"Training...\")\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "\n",
        "        unmasked_weight = unmasked_weight_config['initial'] * (unmasked_weight_config['final'] / unmasked_weight_config['initial']) ** (i / NUM_EPOCHS)\n",
        "\n",
        "        for j, (true_hits, true_pitches, true_vels) in enumerate(train_loader, 1):\n",
        "            true_hits, true_pitches, true_vels = true_hits.to(device), true_pitches.to(device), true_vels.to(device)\n",
        "\n",
        "            optimiser.zero_grad()\n",
        "\n",
        "            div_size = random.choices([1, 2, 4, 8], [0.3, 0.3, 0.2, 0.2])[0]\n",
        "            true_hits = true_hits.tensor_split(div_size, dim=1)\n",
        "            true_pitches = true_pitches.tensor_split(div_size, dim=1)\n",
        "            true_vels = true_vels.tensor_split(div_size, dim=1)\n",
        "            true_hits = torch.cat(true_hits, dim=0)\n",
        "            true_pitches = torch.cat(true_pitches, dim=0)\n",
        "            true_vels = torch.cat(true_vels, dim=0)\n",
        "\n",
        "            mask_prob = torch.randn(1, device=device) * mask_prob_config['std'] + mask_prob_config['mean']\n",
        "            mask_prob = torch.clamp(mask_prob, mask_prob_config['min'], mask_prob_config['max'])\n",
        "            \n",
        "\n",
        "            masked_hits, masked_pitches = true_hits.clone(), true_pitches.clone()\n",
        "\n",
        "            hit_mask = torch.rand_like(true_hits, device=device) < mask_prob\n",
        "            masked_hits[hit_mask] = 0.0\n",
        "            masked_pitches[hit_mask.expand_as(masked_pitches)] = 0.0\n",
        "\n",
        "            model_input = torch.cat((masked_hits, masked_pitches), dim=-1)\n",
        "\n",
        "            pred_hits, pred_pitches, pred_vels = model(model_input)\n",
        "            loss = model.loss_function(\n",
        "                (pred_hits, pred_pitches, pred_vels),\n",
        "                (true_hits, true_pitches, true_vels),\n",
        "                hit_mask,\n",
        "                unmasked_weight=unmasked_weight\n",
        "            )\n",
        "            loss.backward()\n",
        "            optimiser.step()\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "            writer.add_scalar('loss/train/batch', loss.item(), i * len(train_loader) + j)\n",
        "            writer.add_scalar('loss/train/running', total_train_loss / j, i * len(train_loader) + j)\n",
        "\n",
        "            if j % 10 == 0:\n",
        "                print(f'[{j}/{len(train_loader)}] - Running loss = {total_train_loss / j}')\n",
        "                lr_scheduler.step()\n",
        "\n",
        "\n",
        "        # Keep track of training loss\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "        writer.add_scalar('loss/train/epoch', avg_train_loss, i)\n",
        "\n",
        "\n",
        "        print(\"Validation...\")\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for true_hits, true_pitches, true_vels in val_loader:\n",
        "                true_hits, true_pitches, true_vels = true_hits.to(device), true_pitches.to(device), true_vels.to(device)\n",
        "\n",
        "                div_size = random.choices([1, 2, 4, 8], [0.3, 0.3, 0.2, 0.2])[0]\n",
        "                true_hits = true_hits.tensor_split(div_size, dim=1)\n",
        "                true_pitches = true_pitches.tensor_split(div_size, dim=1)\n",
        "                true_vels = true_vels.tensor_split(div_size, dim=1)\n",
        "                true_hits = torch.cat(true_hits, dim=0)\n",
        "                true_pitches = torch.cat(true_pitches, dim=0)\n",
        "                true_vels = torch.cat(true_vels, dim=0)\n",
        "\n",
        "                mask_prob = torch.randn(1, device=device) * mask_prob_config['std'] + mask_prob_config['mean']\n",
        "                mask_prob = torch.clamp(mask_prob, mask_prob_config['min'], mask_prob_config['max'])\n",
        "\n",
        "                # Create same masking pattern as in training\n",
        "                masked_hits, masked_pitches = true_hits.clone(), true_pitches.clone()\n",
        "                hit_mask = torch.rand_like(true_hits, device=device) < mask_prob\n",
        "                masked_hits[hit_mask] = 0.0\n",
        "                masked_pitches[hit_mask.expand_as(masked_pitches)] = 0.0\n",
        "\n",
        "                model_input = torch.cat((masked_hits, masked_pitches), dim=-1)\n",
        "\n",
        "                pred_hits, pred_pitches, pred_vels = model(model_input)\n",
        "                loss = model.loss_function(\n",
        "                    (pred_hits, pred_pitches, pred_vels),\n",
        "                    (true_hits, true_pitches, true_vels),\n",
        "                    hit_mask,\n",
        "                    unmasked_weight=unmasked_weight\n",
        "                )\n",
        "\n",
        "                total_val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "        writer.add_scalar('loss/val/epoch', avg_val_loss, i)\n",
        "\n",
        "        print(f\"Epoch {i+1}/{NUM_EPOCHS}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        writer.add_hparams(hyperparams, {\n",
        "            'train_loss': avg_train_loss,\n",
        "            'val_loss': avg_val_loss,\n",
        "        }, global_step=i)\n",
        "\n",
        "        # Save the model checkpoint\n",
        "        checkpoint_path = os.path.join(log_dir, f'checkpoint_epoch_{i+1}.pth')\n",
        "        torch.save(model.state_dict(), checkpoint_path)\n",
        "        print(f\"Model checkpoint saved to {checkpoint_path}\")\n",
        "\n",
        "        # Save example pattern to TensorBoard\n",
        "        with torch.no_grad():\n",
        "            for name, (prompt_hits, prompt_pitches, prompt_vels) in zip(['BACKBEAT', 'PLEASE', 'AMEN'], [BACKBEAT, PLEASE, AMEN]):\n",
        "                print(f\"Generating example pattern: {name}\")\n",
        "                model_input = torch.cat((prompt_hits, prompt_pitches), dim=-1)\n",
        "                # Save the model to TensorBoard\n",
        "                writer.add_graph(model, model_input)\n",
        "                writer.flush()\n",
        "\n",
        "                hit_logits, pitch_logits, vels = model(model_input)\n",
        "                hit_probs = torch.sigmoid(hit_logits)\n",
        "                pitch_probs = torch.sigmoid(pitch_logits)\n",
        "                hit_preds = (hit_probs > 0.5 + 0.2 * torch.randn_like(hit_probs, device=device)).float()\n",
        "                pitch_preds = (pitch_probs > 0.5 + 0.2 * torch.randn_like(pitch_probs, device=device)).float()\n",
        "\n",
        "                writer.add_histogram(f'example/{name}/hits', hit_probs)\n",
        "                writer.add_histogram(f'example/{name}/pitches', pitch_probs)\n",
        "                writer.add_histogram(f'example/{name}/velocities', vels)\n",
        "\n",
        "                midi = encoder.decode((hit_preds.squeeze(), pitch_preds.squeeze(), vels.squeeze()))\n",
        "                audio, sr = utils.midi_to_audio_tensor(FS, midi)\n",
        "                writer.add_audio(f'example/{name}/audio', audio, sample_rate=sr, global_step=i)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3baf56ae",
      "metadata": {
        "id": "3baf56ae"
      },
      "outputs": [],
      "source": [
        "def make_prompt(length: int, prompt_hits: list[tuple[int, int, float]]):\n",
        "    hits = torch.zeros(1, length, 1)\n",
        "    pitches = torch.zeros(1, length, 9)\n",
        "    velocities = torch.zeros(1, length, 9)\n",
        "\n",
        "    for index, pitch, vel in prompt_hits:\n",
        "        hits[:, index, :] = 1.0\n",
        "        pitches[:, index, pitch] = 1.0\n",
        "        velocities[:, index, pitch] = vel\n",
        "\n",
        "    return hits.to(device), pitches.to(device), velocities.to(device)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else \"cpu\")\n",
        "model = DrumInpaintingTransformer(num_pitches=9, embedding_dim=32, num_layers=20, num_heads=16, pitch_pos_weight=3.0)\n",
        "model.load_state_dict(torch.load('drum_inpainting_model.pth', map_location=device))\n",
        "model = model.to(device)\n",
        "please_prompt = [\n",
        "    (0, drum_encoder.pitch_to_index[36], 0.8),\n",
        "    (3, drum_encoder.pitch_to_index[36], 0.4),\n",
        "    (4, drum_encoder.pitch_to_index[38], 0.8),\n",
        "    (6, drum_encoder.pitch_to_index[36], 0.7),\n",
        "    (11, drum_encoder.pitch_to_index[36], 0.5),\n",
        "    (12, drum_encoder.pitch_to_index[38], 0.8),\n",
        "    (14, drum_encoder.pitch_to_index[36], 0.7),\n",
        "    (19, drum_encoder.pitch_to_index[36], 0.5),\n",
        "    (20, drum_encoder.pitch_to_index[38], 0.8),\n",
        "    (22, drum_encoder.pitch_to_index[36], 0.7),\n",
        "    (27, drum_encoder.pitch_to_index[36], 0.5),\n",
        "    (28, drum_encoder.pitch_to_index[38], 0.8),\n",
        "    (29, drum_encoder.pitch_to_index[38], 0.3),\n",
        "]\n",
        "prompt_hits, prompt_pitches, prompt_vels = make_prompt(32, please_prompt)\n",
        "midi = encoder.decode((prompt_hits.squeeze(), prompt_pitches.squeeze(), prompt_vels.squeeze()))\n",
        "ipd.display(utils.midi_to_audio_display(FS, midi))\n",
        "\n",
        "hit_logits, pitch_logits, vels = model(torch.cat((prompt_hits, prompt_pitches), dim=-1))\n",
        "hit_probs = torch.sigmoid(hit_logits)\n",
        "pitch_probs = torch.sigmoid(pitch_logits)\n",
        "\n",
        "hit_logits = (torch.sigmoid(hit_logits) > 0.3).to(dtype=prompt_hits.dtype)\n",
        "pitch_logits = (torch.sigmoid(pitch_logits) > 0.4).to(dtype=prompt_pitches.dtype)\n",
        "\n",
        "midi = encoder.decode((hit_logits.squeeze(0), pitch_logits.squeeze(0), vels.squeeze(0)))\n",
        "display(utils.midi_to_audio_display(FS, midi))\n",
        "\n",
        "hit_mask = prompt_hits > 0.5\n",
        "merged_hits = prompt_hits.masked_scatter(hit_mask, hit_logits)\n",
        "merged_pitches = prompt_pitches.masked_scatter(hit_mask.expand_as(prompt_pitches), pitch_logits)\n",
        "merged_vels = prompt_vels.masked_scatter(hit_mask.expand_as(prompt_vels), vels)\n",
        "\n",
        "midi = encoder.decode((merged_hits.squeeze(), merged_pitches.squeeze(), merged_vels.squeeze()))\n",
        "display(utils.midi_to_audio_display(FS, midi))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b31adb0a",
      "metadata": {
        "id": "b31adb0a"
      },
      "source": [
        "# Interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8deddda4",
      "metadata": {
        "id": "8deddda4"
      },
      "outputs": [],
      "source": [
        "drum_hits = {\n",
        "    'Kick': 36,\n",
        "    'Snare': 38,\n",
        "    'Closed Hi-Hat': 42,\n",
        "    'Open Hi-Hat': 46,\n",
        "    'Low Tom': 43,\n",
        "    'Mid Tom': 47,\n",
        "    'High Tom': 50,\n",
        "    'Crash': 49,\n",
        "    'Ride': 51\n",
        "}\n",
        "\n",
        "# Define drum colors dictionary\n",
        "drum_colors = {\n",
        "    'Kick': '#FF5252',      # Red\n",
        "    'Snare': '#FFEB3B',     # Yellow\n",
        "    'Closed Hi-Hat': '#2196F3', # Blue\n",
        "    'Open Hi-Hat': '#03A9F4',  # Light Blue\n",
        "    'Low Tom': '#FF9800',   # Orange\n",
        "    'Mid Tom': '#FF7043',   # Deep Orange\n",
        "    'High Tom': '#F44336',  # Red\n",
        "    'Crash': '#9C27B0',     # Purple\n",
        "    'Ride': '#673AB7'       # Deep Purple\n",
        "}\n",
        "\n",
        "\n",
        "class DrumSequencer:\n",
        "    def __init__(self, model: DrumInpaintingTransformer, encoder: tokeniser.DrumSequenceEncoder, fluid_synth: FluidSynth, num_steps: int = 16):\n",
        "        self.model = model\n",
        "        self.encoder = encoder\n",
        "        self.fs = fluid_synth\n",
        "        self.device = next(model.parameters()).device\n",
        "\n",
        "        self.num_steps = num_steps\n",
        "\n",
        "        self._create_interface()\n",
        "\n",
        "    def _create_interface(self):\n",
        "        # Create main layout\n",
        "        self.output = widgets.Output()\n",
        "        self.drum_pads = {}\n",
        "\n",
        "        # Create grid container\n",
        "        grid = widgets.GridspecLayout(\n",
        "            len(drum_hits) + 1,\n",
        "            self.num_steps + 1,\n",
        "            grid_gap='2px',\n",
        "            width='100%',\n",
        "            height=f'{len(drum_hits)*40}px'\n",
        "        )\n",
        "\n",
        "        for i in range(1, self.num_steps + 1):\n",
        "            grid[0, i] = widgets.Label(\n",
        "                f'{(i - 1) // 4 + 1}.{(i - 1) % 4 + 1}',\n",
        "                layout=widgets.Layout(margin='auto', justify_content='center')\n",
        "            )\n",
        "\n",
        "        # Add drum checkboxes\n",
        "        for i, (drum_name, pitch) in enumerate(drum_hits.items(), 1):\n",
        "            # Add label for the drum\n",
        "            label = widgets.Label(\n",
        "                drum_name,\n",
        "                align='center',\n",
        "                layout=widgets.Layout(\n",
        "                    height='auto',\n",
        "                    width='100px',\n",
        "                    padding='3px',\n",
        "                    justify_content='flex-end',\n",
        "            ))\n",
        "            grid[i, 0] = label\n",
        "\n",
        "            # Create checkbox row for this drum\n",
        "            self.drum_pads[drum_name] = []\n",
        "            for step in range(1, self.num_steps + 1):\n",
        "                pad = widgets.ToggleButton(\n",
        "                    value=False,\n",
        "                    description='',\n",
        "                    tooltip=f'{drum_name} Step {step}',\n",
        "                    disabled=False,\n",
        "                    button_style='',\n",
        "                    layout=widgets.Layout(\n",
        "                        width='30px',\n",
        "                        height='30px',\n",
        "                        padding='0px',\n",
        "                        margin='auto',\n",
        "                        justify_content='center',\n",
        "                        align_items='center',\n",
        "                        border='2px solid #888',\n",
        "                        border_radius='4px'\n",
        "                    )\n",
        "                )\n",
        "                self.drum_pads[drum_name].append(pad)\n",
        "                grid[i, step] = pad\n",
        "\n",
        "        # Add sliders for controlling generation parameters\n",
        "        density_slider = widgets.FloatSlider(\n",
        "            value=2.0, \n",
        "            min=1.0,\n",
        "            max=5.0, \n",
        "            step=0.01, \n",
        "            description='Density:', \n",
        "            tooltip='Controls how many hits will be generated (higher == more hits)',\n",
        "            layout=widgets.Layout(width='300px'),\n",
        "            readout=False\n",
        "        )\n",
        "        \n",
        "        diversity_slider = widgets.FloatSlider(\n",
        "            value=2.0,\n",
        "            min=1.0, \n",
        "            max=5.0, \n",
        "            step=0.01, \n",
        "            description='Diversity:', \n",
        "            tooltip='Controls variety of drum types (higher == more variety)',\n",
        "            layout=widgets.Layout(width='300px'),\n",
        "            readout=False\n",
        "        )\n",
        "        \n",
        "        tempo_slider = widgets.IntSlider(\n",
        "            value=120, \n",
        "            min=60, \n",
        "            max=200, \n",
        "            step=1, \n",
        "            description='Tempo:',\n",
        "            tooltip='Controls playback speed',\n",
        "            layout=widgets.Layout(width='300px'),\n",
        "            readout=False\n",
        "        )\n",
        "\n",
        "        parameter_controls = widgets.HBox(\n",
        "            [density_slider, diversity_slider, tempo_slider],\n",
        "            layout = widgets.Layout(\n",
        "                justify_content='space-around',\n",
        "                width='100%',\n",
        "                height='auto',\n",
        "                padding='auto',\n",
        "                margin='auto',\n",
        "                align_items='center',\n",
        "            )\n",
        "        )\n",
        "        \n",
        "        self.density_slider = density_slider\n",
        "        self.diversity_slider = diversity_slider\n",
        "        self.tempo_slider = tempo_slider\n",
        "\n",
        "        # Add control buttons\n",
        "        play_btn = widgets.Button(description='Play Pattern')\n",
        "        play_btn.on_click(self._play_pattern)\n",
        "\n",
        "        suggest_btn = widgets.Button(description='Suggest Pattern')\n",
        "        suggest_btn.on_click(self._suggest_pattern)\n",
        "\n",
        "        generate_btn = widgets.Button(description='Fill Pattern')\n",
        "        generate_btn.on_click(self._generate_full_pattern)\n",
        "\n",
        "        clear_btn = widgets.Button(description='Clear')\n",
        "        clear_btn.on_click(self._clear_pattern)\n",
        "\n",
        "        save_btn = widgets.Button(description='Save')\n",
        "        save_btn.on_click(self._save_pattern)\n",
        "\n",
        "        # Arrange the elements\n",
        "        controls = widgets.HBox(\n",
        "            [play_btn, suggest_btn, generate_btn, clear_btn, save_btn],\n",
        "            layout=widgets.Layout(\n",
        "                justify_content='space-around',\n",
        "                width='100%',\n",
        "                height='auto',\n",
        "                padding='auto',\n",
        "                margin='auto',\n",
        "                align_items='center',\n",
        "            )\n",
        "        )\n",
        "\n",
        "        main_layout = widgets.VBox(\n",
        "            [grid, parameter_controls, controls, self.output],\n",
        "            layout=widgets.Layout(\n",
        "                justify_content='space-around',\n",
        "                width='100%',\n",
        "                height='auto',\n",
        "                padding='10px',\n",
        "                margin='20px 0',\n",
        "                align_items='center',\n",
        "                grid_gap='15px'\n",
        "            )\n",
        "        )\n",
        "        display(main_layout)\n",
        "\n",
        "    def _make_midi_from_widgets(self):\n",
        "        track = symusic.Track(is_drum=True)\n",
        "\n",
        "        tpq = self.encoder.tpq\n",
        "        subdiv = self.encoder.subdivision\n",
        "\n",
        "        for drum, boxes in self.drum_pads.items():\n",
        "            for i, box in enumerate(boxes):\n",
        "                if box.value:\n",
        "                    track.notes.append(symusic.Note(int(i * tpq * 4 / subdiv), 80, drum_hits[drum], 80))\n",
        "\n",
        "        track.sort()\n",
        "        midi = symusic.Score(tpq)\n",
        "        midi.tracks.append(track)\n",
        "        return midi\n",
        "\n",
        "    def _play_pattern(self, _):\n",
        "        hits, pitches = self._make_tensors_from_widgets()\n",
        "        _, _, vels = self.model(torch.cat((hits, pitches), dim=-1))\n",
        "\n",
        "        midi = self.encoder.decode((hits.squeeze(), pitches.squeeze(), vels.squeeze()))\n",
        "        self._display_midi(midi)\n",
        "\n",
        "    def _make_tensors_from_widgets(self):\n",
        "        hits = torch.zeros((1, self.num_steps, 1), device=device)\n",
        "        pitches = torch.zeros((1, self.num_steps, len(drum_hits)), device=device)\n",
        "\n",
        "        for drum, boxes in self.drum_pads.items():\n",
        "            for i, box in enumerate(boxes):\n",
        "                if box.value:\n",
        "                    hits[:, i, :] = 1.0\n",
        "                    pitches[:, i, self.encoder.pitch_to_index[drum_hits[drum]]] = 1.0\n",
        "\n",
        "        return hits, pitches\n",
        "    \n",
        "    def _get_beta_dist(self):\n",
        "        # Density for increasing likelihood of notes (centre of beta distribution)\n",
        "        # Scale to fit beta distribution (restriction alpha, beta > 1.0)\n",
        "        density = self.density_slider.value / 2.0\n",
        "\n",
        "        # Diversity for increasing variety of pitches (spread of beta distribution)\n",
        "        # Scale to fit beta distribution (restriction alpha, beta > 1.0)\n",
        "        diversity = self.density_slider.value / 20.0\n",
        "        \n",
        "        denom = (diversity * (density + 1.0))\n",
        "        alpha = 1.0 / denom\n",
        "        beta = density / denom\n",
        "\n",
        "        dist = torch.distributions.Beta(alpha, beta)\n",
        "        return dist\n",
        "        \n",
        "    def _suggest_pattern(self, _):\n",
        "        hits, pitches = self._make_tensors_from_widgets()\n",
        "        pred_hits, pred_pitches, _ = model(torch.cat((hits, pitches), dim=-1))\n",
        "        pred_hits = torch.sigmoid(pred_hits)\n",
        "        pred_pitches = torch.sigmoid(pred_pitches)\n",
        "\n",
        "        dist = self._get_beta_dist()\n",
        "\n",
        "        new_pitches = pred_hits.where(pred_pitches > dist.sample(pred_pitches.shape).to(device), torch.zeros_like(pred_pitches, device=device)).squeeze()\n",
        "\n",
        "        for drum, boxes in self.drum_pads.items():\n",
        "            pitch_idx = self.encoder.pitch_to_index[drum_hits[drum]]\n",
        "            for step, box in enumerate(boxes):\n",
        "                box.value = (new_pitches[step, pitch_idx] > 0.5).item()\n",
        "\n",
        "        self._play_pattern(_)\n",
        "\n",
        "    def _generate_full_pattern(self, _):\n",
        "        # Take current pattern and copy to N bars\n",
        "        hits, pitches = self._make_tensors_from_widgets()\n",
        "        hits = hits.repeat((1, 4, 1))\n",
        "        pitches = pitches.repeat((1, 4, 1))\n",
        "        with self.output:\n",
        "            self.output.clear_output()\n",
        "            print(\"Hits:\", hits)\n",
        "            print(\"Pitches:\", pitches)\n",
        "        \n",
        "        hits, pitches, vels = model(torch.cat((hits, pitches), dim=-1))\n",
        "        hits = torch.sigmoid(hits)\n",
        "        pitches = torch.sigmoid(pitches)\n",
        "        \n",
        "        pitches = hits * pitches\n",
        "\n",
        "        dist = self._get_beta_dist()\n",
        "        new_pitches = hits.where(pitches > dist.sample(pitches.shape).to(device), torch.zeros_like(pitches, device=device)).squeeze()\n",
        "\n",
        "        midi = self.encoder.decode((hits.squeeze(), new_pitches.squeeze(), vels.squeeze()))\n",
        "        self._display_midi(midi)\n",
        "    \n",
        "    def _display_midi(self, midi: symusic.types.Score):\n",
        "        midi.tempos = [symusic.Tempo(0, self.tempo_slider.value)]\n",
        "        self.last_midi = midi\n",
        "        with self.output:\n",
        "            audio = utils.midi_to_audio_display(self.fs, midi)\n",
        "            ipd.clear_output()\n",
        "            display(audio)\n",
        "    \n",
        "    def _save_pattern(self, _):\n",
        "        if hasattr(self, 'last_midi'):\n",
        "            output_dir = os.path.join('output', 'midis')\n",
        "            os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "            # Generate a unique filename with timestamp\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            filename = f\"drum_pattern_{timestamp}.mid\"\n",
        "            file_path = os.path.join(output_dir, filename)\n",
        "            \n",
        "            # Save the MIDI file\n",
        "            self.last_midi.dump_midi(file_path)\n",
        "            with self.output:\n",
        "                self.output.clear_output()\n",
        "                print(f\"Pattern saved to {file_path}\")\n",
        "                audio = utils.midi_to_audio_display(self.fs, self.last_midi)\n",
        "                display(audio)\n",
        "        else:\n",
        "            with self.output:\n",
        "                self.output.clear_output()\n",
        "                print(\"No pattern to save. Please generate a pattern first.\")\n",
        "\n",
        "\n",
        "    def _clear_pattern(self, _):\n",
        "        for boxes in self.drum_pads.values():\n",
        "            for box in boxes:\n",
        "                box.value = False\n",
        "\n",
        "DrumSequencer(model, drum_encoder, FS)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
