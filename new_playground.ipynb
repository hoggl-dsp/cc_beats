{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e668adaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "IS_COLAB = is_colab()\n",
    "\n",
    "if IS_COLAB:\n",
    "  !pip install git+https://github.com/hoggl-dsp/cc_beats\n",
    "\n",
    "  !apt-get install fluidsynth\n",
    "  !mkdir data\n",
    "  !mkdir data/sf2\n",
    "  !wget -O data/sf2/Xpand_2_-_Practice_Room_Kit.sf2 'https://musical-artifacts.com/artifacts/6296/Xpand_2_-_Practice_Room_Kit.sf2'\n",
    "\n",
    "  !wget -O data/groove_midi_only.zip 'https://storage.googleapis.com/magentadata/datasets/groove/groove-v1.0.0-midionly.zip'\n",
    "  !unzip data/groove_midi_only.zip -d data/\n",
    "\n",
    "  !wget -O data/clean_midi.tar.gz 'http://hog.ee.columbia.edu/craffel/lmd/clean_midi.tar.gz'\n",
    "  !tar -xf data/clean_midi.tar.gz -C data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8fbf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import symusic\n",
    "import symusic.types\n",
    "from midi2audio import FluidSynth\n",
    "\n",
    "import IPython.display as ipd\n",
    "import ipywidgets as widgets\n",
    "import tqdm\n",
    "import os\n",
    "\n",
    "from typing import Callable\n",
    "\n",
    "from cc_beats import tokeniser, modules, utils\n",
    "\n",
    "sf_path = 'data/sf2/Xpand_2_-_Practice_Room_Kit.sf2'\n",
    "FS = FluidSynth(sf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e76589",
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_files = []\n",
    "for root, _, files in os.walk(os.path.join('data', 'groove')):\n",
    "    midi_files.extend([os.path.join(root, file) for file in files if file.endswith('.mid') and 'beat' in file and 'eval' not in os.path.basename(root)])\n",
    "\n",
    "midi_files = []\n",
    "for root, _, files in os.walk(os.path.join('data', 'clean_midi')):\n",
    "    midi_files.extend([os.path.join(root, file) for file in files if file.endswith('.mid')]) # and 'beat' in file and 'eval' not in os.path.basename(root)])\n",
    "\n",
    "for i in range(20):\n",
    "    print(midi_files[i])\n",
    "\n",
    "midi_file = symusic.Score(midi_files[0])\n",
    "\n",
    "encoder = tokeniser.DrumSequenceEncoder(subdivision=16)\n",
    "\n",
    "tok_sequence = encoder.encode(midi_file)\n",
    "returned_midi = encoder.decode(tok_sequence)\n",
    "\n",
    "returned_midi.time_signatures.extend(midi_file.time_signatures)\n",
    "returned_midi.key_signatures.extend(midi_file.key_signatures)\n",
    "returned_midi.tempos.extend(midi_file.tempos)\n",
    "\n",
    "print(\"Actual File\")\n",
    "print(midi_file)\n",
    "print(midi_file.tracks[0].notes[:10])\n",
    "print('-----------------------------')\n",
    "\n",
    "print(\"Decoded File\")\n",
    "print(returned_midi)\n",
    "print(returned_midi.tracks[0].notes[:10])\n",
    "print('-----------------------------')\n",
    "\n",
    "display(utils.midi_to_audio_display(FS, midi_file))\n",
    "display(utils.midi_to_audio_display(FS, returned_midi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e020893b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_midi_data_files(root_dir: str, filter_fn: Callable[[str, str], bool] = lambda root, file: True):\n",
    "    midi_files = []\n",
    "    for root, _, files in os.walk(root_dir):\n",
    "        midi_files.extend([os.path.join(root, file) for file in files if filter_fn(root, file)])\n",
    "    return midi_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ebb235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lakh clean_set analysis\n",
    "import pandas as pd\n",
    "\n",
    "def do_lakh_analysis():\n",
    "    files = get_midi_data_files(os.path.join('data', 'clean_midi'))\n",
    "\n",
    "    data = {\n",
    "        'File': [],\n",
    "        'File Length (quarters)': [],\n",
    "        'Tempos': [],\n",
    "        'Is 4/4': [],\n",
    "        'Num Drum Tracks': [],\n",
    "        'Drum Hit Count': [],\n",
    "        'Unique Drum Hits': [],\n",
    "        'First Drum (quarters)': [],\n",
    "        'Last Drum (quarters)': [],\n",
    "    }\n",
    "    for file in tqdm.tqdm(files, desc='Analysing Midi Files'):\n",
    "        midi = None\n",
    "        drum_tracks = None\n",
    "        data['File'].append(file)\n",
    "        try:\n",
    "            midi = symusic.Score.from_file(file)\n",
    "            data['File Length (quarters)'].append((midi.end() - midi.start()) / midi.tpq)\n",
    "            data['Tempos'].append(midi.tempos)\n",
    "            data['Is 4/4'].append(len(midi.time_signatures) == 1 and (midi.time_signatures[0].numerator == 4 and midi.time_signatures[0].denominator == 4))\n",
    "\n",
    "            drum_tracks = [track for track in midi.tracks if track.is_drum]\n",
    "            data['Num Drum Tracks'].append(len(drum_tracks))\n",
    "\n",
    "            count = 0\n",
    "            note_set = set()\n",
    "            first_note_time = None\n",
    "            last_note_time = None\n",
    "            for track in drum_tracks:\n",
    "                for note in track.notes:\n",
    "                    if note.pitch not in tokeniser.ROLAND_DRUM_MAPPING:\n",
    "                        continue\n",
    "                    count += 1\n",
    "                    note_set.add(note.pitch)\n",
    "                    if first_note_time is None or first_note_time > note.time:\n",
    "                        first_note_time = note.time\n",
    "                    if last_note_time is None or last_note_time < note.time:\n",
    "                        last_note_time = note.time\n",
    "            \n",
    "            data['Drum Hit Count'].append(count)\n",
    "            data['Unique Drum Hits'].append(note_set)\n",
    "            data['First Drum (quarters)'].append(first_note_time / midi.tpq if first_note_time is not None else None)\n",
    "            data['Last Drum (quarters)'].append(last_note_time / midi.tpq if last_note_time is not None else None)\n",
    "\n",
    "        except:\n",
    "            for key in data:\n",
    "                if key != 'File':\n",
    "                    data[key].append(None)\n",
    "    \n",
    "\n",
    "    \n",
    "    lakh_df = pd.DataFrame(data)\n",
    "\n",
    "    print(\" ***** Filtering Dataset *****\")\n",
    "    print(\"Full Dataset:\", len(lakh_df))\n",
    "    lakh_df = lakh_df[~lakh_df.isna()]\n",
    "    print(\"Without unreadable files:\", len(lakh_df))\n",
    "    lakh_df = lakh_df[lakh_df['Num Drum Tracks'] > 0]\n",
    "    print(\"Without files with no drums:\", len(lakh_df))\n",
    "    lakh_df = lakh_df[lakh_df['Is 4/4']]\n",
    "    print(\"Without files not in 4/4:\", len(lakh_df))\n",
    "    lakh_df = lakh_df[lakh_df['Drum Hit Count'] > 256]\n",
    "    print(\"Without files with less than 256 drum hits\", len(lakh_df))\n",
    "    lakh_df = lakh_df[lakh_df['Drum Hit Count'] / (lakh_df['File Length (quarters)']) >= 1.0]\n",
    "    print(\"Without files with note density < 1 per quarter:\", len(lakh_df))\n",
    "    lakh_df = lakh_df[lakh_df['File Length (quarters)'] < 1000.0]\n",
    "    print(\"Without files with > 1000 beats:\", len(lakh_df))\n",
    "    lakh_df = lakh_df[lakh_df['Unique Drum Hits'].apply(lambda x: len(x) > 2)]\n",
    "    print(\"Without files with no drum hit variety:\", len(lakh_df))\n",
    "\n",
    "    return lakh_df\n",
    "    \n",
    "lakh_clean_df = do_lakh_analysis()\n",
    "\n",
    "print(lakh_clean_df.describe())\n",
    "\n",
    "print(lakh_clean_df.head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1676ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrumInpaintingTransformer(torch.nn.Module):\n",
    "    def __init__(self, num_pitches: int, embedding_dim: int = 64, num_layers: int = 6, num_heads: int = 8, pitch_pos_weight: float = 2.0, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.input_proj = torch.nn.Linear(1 + num_pitches, embedding_dim)\n",
    "\n",
    "        self.transformer = torch.nn.ModuleList([\n",
    "            modules.TransformerLayerWithRelativeAttention(embedding_dim, num_heads=num_heads, dropout=0.1, max_distance=64)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.output_hit_proj = torch.nn.Linear(embedding_dim, 1)\n",
    "        self.output_pitches_proj = torch.nn.Linear(embedding_dim, num_pitches)\n",
    "        self.output_velocities_proj = torch.nn.Linear(embedding_dim, num_pitches)\n",
    "\n",
    "        if isinstance(pitch_pos_weight, float):\n",
    "            pitch_pos_weight = torch.ones((1, 1, 9)) * pitch_pos_weight\n",
    "        self.pitch_pos_weight = pitch_pos_weight\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, attention_mask=None):\n",
    "        # Input dims [batch, seq_len, 1 + 2 * num_pitches]\n",
    "        \n",
    "        x = self.input_proj(x) # [batch, seq_len, embedding_dim]\n",
    "\n",
    "        for layer in self.transformer:\n",
    "            x = layer(x) # [batch, seq_len, embedding_dim]\n",
    "        \n",
    "        hit_logits = self.output_hit_proj(x)\n",
    "        pitch_logits = self.output_pitches_proj(x)\n",
    "        vel_logits = torch.sigmoid(self.output_velocities_proj(x))\n",
    "\n",
    "        return hit_logits, pitch_logits, vel_logits\n",
    "    \n",
    "    def loss_function(self, \n",
    "            preds: tuple[torch.Tensor, torch.Tensor, torch.Tensor], \n",
    "            truths: tuple[torch.Tensor, torch.Tensor, torch.Tensor],\n",
    "            inpainting_mask: torch.Tensor,\n",
    "            unmasked_weight: float,\n",
    "        ) -> torch.Tensor:\n",
    "        pred_hits, pred_pitches, pred_vels = preds\n",
    "        true_hits, true_pitches, true_vels = truths\n",
    "        \n",
    "        device = pred_hits.device\n",
    "\n",
    "\n",
    "        hit_weights = torch.ones_like(true_hits)\n",
    "        hit_weights[~inpainting_mask] = unmasked_weight\n",
    "\n",
    "        hit_loss = torch.nn.functional.binary_cross_entropy_with_logits(\n",
    "            pred_hits,\n",
    "            true_hits,\n",
    "            weight=hit_weights\n",
    "        )\n",
    "\n",
    "        # hit_loss = self.hit_loss(pred_hits[inpainting_mask], true_hits[inpainting_mask])\n",
    "\n",
    "        hit_mask = inpainting_mask & (true_hits > 0.5)\n",
    "        hit_mask = hit_mask.expand_as(true_pitches)\n",
    "\n",
    "        pitch_weights = torch.ones_like(true_pitches)\n",
    "        pitch_weights[~hit_mask] = unmasked_weight\n",
    "\n",
    "        pitch_loss = torch.nn.functional.binary_cross_entropy_with_logits(\n",
    "            pred_pitches,\n",
    "            true_pitches,\n",
    "            weight=pitch_weights,\n",
    "            pos_weight=self.pitch_pos_weight.to(pred_pitches.device)\n",
    "        )\n",
    "\n",
    "        hit_pitch_mask = hit_mask & (true_pitches > 0.5)\n",
    "        vel_loss = torch.nn.functional.mse_loss(\n",
    "            pred_vels[hit_pitch_mask], \n",
    "            true_vels[hit_pitch_mask],\n",
    "            # reduction='sum'\n",
    "        ) if hit_pitch_mask.any() else torch.tensor(0.0, device=device)\n",
    "\n",
    "        return hit_loss + pitch_loss + vel_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869a25ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "def make_dataset(encoder: tokeniser.DrumSequenceEncoder, midi_files: list[str], max_seq_length: int = 64):\n",
    "    sequences = encoder.encode_all(midi_files)\n",
    "    split_hits = []\n",
    "    split_pitches = []\n",
    "    split_velocities = []\n",
    "    for hits, pitches, velocities in sequences:\n",
    "        len_padding = max_seq_length - (hits.size(0) % max_seq_length)\n",
    "        if len_padding > 0:\n",
    "            hits = torch.cat([hits, torch.zeros(len_padding, hits.size(1), dtype=hits.dtype, device=hits.device)])\n",
    "            pitches = torch.cat([pitches, torch.zeros(len_padding, pitches.size(1), dtype=pitches.dtype, device=pitches.device)])\n",
    "            velocities = torch.cat([velocities, torch.zeros(len_padding, velocities.size(1), dtype=velocities.dtype, device=velocities.device)])\n",
    "        \n",
    "        split_hits.extend(torch.tensor_split(hits, int(hits.size(0) / max_seq_length)))\n",
    "        split_pitches.extend(torch.tensor_split(pitches, int(pitches.size(0) / max_seq_length)))\n",
    "        split_velocities.extend(torch.tensor_split(velocities, int(velocities.size(0) / max_seq_length)))\n",
    "    \n",
    "    return torch.utils.data.TensorDataset(\n",
    "        torch.stack(split_hits, dim=0),\n",
    "        torch.stack(split_pitches, dim=0),\n",
    "        torch.stack(split_velocities, dim=0)\n",
    "    )\n",
    "\n",
    "drum_encoder = tokeniser.DrumSequenceEncoder(subdivision=16)\n",
    "dataset = make_dataset(drum_encoder, lakh_clean_df['File'].to_list(), max_seq_length=128)\n",
    "\n",
    "print(\"Dataset_Size:\", len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd22dc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training parameters\n",
    "num_epochs = 1\n",
    "batch_size = 32\n",
    "mask_prob = 0.4\n",
    "unmasked_weight = 0.3\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "model = DrumInpaintingTransformer(num_pitches=9, embedding_dim=16, num_layers=20, num_heads=16, pitch_pos_weight=3.0)\n",
    "model = model.to(device)  # Move model to the appropriate device\n",
    "\n",
    "# Create a DataLoader for batching\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_set, valid_set = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=2e-5, weight_decay=1e-6)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    valid_set, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimiser, T_max=(num_epochs * len(train_loader)) // 10)\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for j, (true_hits, true_pitches, true_vels) in enumerate(train_loader, 1):\n",
    "        optimiser.zero_grad()\n",
    "        \n",
    "        true_hits, true_pitches, true_vels = true_hits.to(device), true_pitches.to(device), true_vels.to(device)\n",
    "        masked_hits, masked_pitches = true_hits.clone(), true_pitches.clone()\n",
    "\n",
    "        hit_mask = torch.rand_like(true_hits, device=device) < mask_prob\n",
    "        masked_hits[hit_mask] = 0.0\n",
    "        masked_pitches[hit_mask.expand_as(masked_pitches)] = 0.0\n",
    "\n",
    "        model_input = torch.cat((masked_hits, masked_pitches), dim=-1)\n",
    "\n",
    "        pred_hits, pred_pitches, pred_vels = model(model_input)\n",
    "        loss = model.loss_function(\n",
    "            (pred_hits, pred_pitches, pred_vels),\n",
    "            (true_hits, true_pitches, true_vels),\n",
    "            hit_mask,\n",
    "            unmasked_weight=unmasked_weight\n",
    "        )\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        if j % 10 == 0:\n",
    "            print(f'[{j}/{len(train_loader)}] - Running loss = {total_train_loss / j}')\n",
    "            lr_scheduler.step()\n",
    "\n",
    "    \n",
    "    # Keep track of training loss\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for true_hits, true_pitches, true_vels in val_loader:\n",
    "            true_hits, true_pitches, true_vels = true_hits.to(device), true_pitches.to(device), true_vels.to(device)\n",
    "            \n",
    "            # Create same masking pattern as in training\n",
    "            masked_hits, masked_pitches = true_hits.clone(), true_pitches.clone()\n",
    "            hit_mask = torch.rand_like(true_hits, device=device) < mask_prob\n",
    "            masked_hits[hit_mask] = 0.0\n",
    "            masked_pitches[hit_mask.expand_as(masked_pitches)] = 0.0\n",
    "            \n",
    "            model_input = torch.cat((masked_hits, masked_pitches), dim=-1)\n",
    "            \n",
    "            pred_hits, pred_pitches, pred_vels = model(model_input)\n",
    "            loss = model.loss_function(\n",
    "                (pred_hits, pred_pitches, pred_vels),\n",
    "                (true_hits, true_pitches, true_vels),\n",
    "                hit_mask,\n",
    "                unmasked_weight=unmasked_weight\n",
    "            )\n",
    "            \n",
    "            total_val_loss += loss.item()\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    \n",
    "    print(f\"Epoch {i+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, 'b-', label='Training Loss')\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, 'r-', label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Save the model to disk\n",
    "torch.save(model.state_dict(), 'drum_inpainting_model.pth')\n",
    "print(\"Model saved to 'drum_inpainting_model.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baf56ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(length: int, prompt_hits: list[tuple[int, int, float]]):\n",
    "    hits = torch.zeros(1, length, 1)\n",
    "    pitches = torch.zeros(1, length, 9)\n",
    "\n",
    "    for index, pitch, velocities in prompt_hits:\n",
    "        hits[:, index, :] = 1.0\n",
    "        pitches[:, index, pitch] = 1.0\n",
    "\n",
    "    return hits.to(device), pitches.to(device), velocities.to(device)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = DrumInpaintingTransformer(num_pitches=9, embedding_dim=32, num_layers=20, num_heads=16, pitch_pos_weight=3.0)\n",
    "model.load_state_dict(torch.load('drum_inpainting_model.pth', map_location=device))\n",
    "model = model.to(device)\n",
    "prompt = [\n",
    "    (0, drum_encoder.pitch_to_index[36], 0.8),\n",
    "    (3, drum_encoder.pitch_to_index[36], 0.4),\n",
    "    (4, drum_encoder.pitch_to_index[38], 0.8),\n",
    "    (6, drum_encoder.pitch_to_index[36], 0.7),\n",
    "    (11, drum_encoder.pitch_to_index[36], 0.5),\n",
    "    (12, drum_encoder.pitch_to_index[38], 0.8),\n",
    "    (14, drum_encoder.pitch_to_index[36], 0.7),\n",
    "    (19, drum_encoder.pitch_to_index[36], 0.5),\n",
    "    (20, drum_encoder.pitch_to_index[38], 0.8),\n",
    "    (22, drum_encoder.pitch_to_index[36], 0.7),\n",
    "    (27, drum_encoder.pitch_to_index[36], 0.5),\n",
    "    (28, drum_encoder.pitch_to_index[38], 0.8),\n",
    "    (29, drum_encoder.pitch_to_index[38], 0.3),\n",
    "]\n",
    "prompt_hits, prompt_pitches, prompt_vels = make_prompt(32, prompt)\n",
    "midi = encoder.decode((prompt_hits.squeeze(), prompt_pitches.squeeze(), prompt_vels.squeeze()))\n",
    "ipd.display(utils.midi_to_audio_display(FS, midi))\n",
    "\n",
    "hit_logits, pitch_logits, vels = model(torch.cat((prompt_hits, prompt_pitches), dim=-1))\n",
    "hit_probs = torch.sigmoid(hit_logits)\n",
    "pitch_probs = torch.sigmoid(pitch_logits)\n",
    "\n",
    "hit_logits = (torch.sigmoid(hit_logits) > 0.3).to(dtype=prompt_hits.dtype)\n",
    "pitch_logits = (torch.sigmoid(pitch_logits) > 0.4).to(dtype=prompt_pitches.dtype)\n",
    "\n",
    "midi = encoder.decode((hit_logits.squeeze(0), pitch_logits.squeeze(0), vels.squeeze(0)))\n",
    "display(utils.midi_to_audio_display(FS, midi))\n",
    "\n",
    "hit_mask = prompt_hits > 0.5\n",
    "merged_hits = prompt_hits.masked_scatter(hit_mask, hit_logits)\n",
    "merged_pitches = prompt_pitches.masked_scatter(hit_mask.expand_as(prompt_pitches), pitch_logits)\n",
    "merged_vels = prompt_vels.masked_scatter(hit_mask.expand_as(prompt_vels), vels)\n",
    "\n",
    "midi = encoder.decode((merged_hits.squeeze(), merged_pitches.squeeze(), merged_vels.squeeze()))\n",
    "display(utils.midi_to_audio_display(FS, midi))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31adb0a",
   "metadata": {},
   "source": [
    "# Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8deddda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "drum_hits = {\n",
    "    'Kick': 36,\n",
    "    'Snare': 38,\n",
    "    'Closed Hi-Hat': 42,\n",
    "    'Open Hi-Hat': 46,\n",
    "    'Low Tom': 43,\n",
    "    'Mid Tom': 47,\n",
    "    'High Tom': 50,\n",
    "    'Crash': 49,\n",
    "    'Ride': 51\n",
    "}\n",
    "\n",
    "class DrumSequencer:\n",
    "    def __init__(self, model: DrumInpaintingTransformer, encoder: tokeniser.DrumSequenceEncoder, fluid_synth: FluidSynth, num_steps: int = 16):\n",
    "        self.model = model\n",
    "        self.encoder = encoder\n",
    "        self.fs = fluid_synth\n",
    "        self.device = next(model.parameters()).device\n",
    "\n",
    "        self.num_steps = num_steps\n",
    "\n",
    "        self._create_interface()\n",
    "\n",
    "    def _create_interface(self):\n",
    "        # Create main layout\n",
    "        self.output = widgets.Output()\n",
    "        self.drum_checkboxes = {}\n",
    "\n",
    "        # Create grid container\n",
    "        grid = widgets.GridspecLayout(\n",
    "            len(drum_hits) + 1, \n",
    "            self.num_steps + 1,                             \n",
    "            grid_gap='2px',\n",
    "            width='100%',\n",
    "            height=f'{len(drum_hits)*40}px'\n",
    "        )\n",
    "\n",
    "        for i in range(1, self.num_steps + 1):\n",
    "            grid[0, i] = widgets.Label(\n",
    "                f'{i}',\n",
    "                layout=widgets.Layout(margin='auto', justify_content='center')\n",
    "            )\n",
    "\n",
    "        # Add drum checkboxes\n",
    "        for i, (drum_name, pitch) in enumerate(drum_hits.items(), 1):\n",
    "            # Add label for the drum\n",
    "            label = widgets.Label(\n",
    "                drum_name,\n",
    "                align='center',\n",
    "                layout=widgets.Layout(\n",
    "                    height='auto',\n",
    "                    width='100px',\n",
    "                    padding='3px',\n",
    "                    justify_content='flex-end',\n",
    "            ))\n",
    "            grid[i, 0] = label\n",
    "            \n",
    "            # Create checkbox row for this drum\n",
    "            self.drum_checkboxes[drum_name] = []\n",
    "            for step in range(1, self.num_steps + 1):\n",
    "                checkbox = widgets.Checkbox(\n",
    "                    value=False,\n",
    "                    indent=False,\n",
    "                    layout=widgets.Layout(\n",
    "                        height='auto', \n",
    "                        width='auto', \n",
    "                        margin='auto',\n",
    "                        justify_content='center'\n",
    "                    )\n",
    "                )\n",
    "                self.drum_checkboxes[drum_name].append(checkbox)\n",
    "                grid[i, step] = checkbox\n",
    "        \n",
    "        \n",
    "                \n",
    "        # Add control buttons\n",
    "        play_btn = widgets.Button(description='Play')\n",
    "        play_btn.on_click(self._play_pattern)\n",
    "\n",
    "        gen_btn = widgets.Button(description='Generate')\n",
    "        gen_btn.on_click(self._generate_pattern)\n",
    "                \n",
    "        clear_btn = widgets.Button(description='Clear')\n",
    "        clear_btn.on_click(self._clear_pattern)\n",
    "                \n",
    "        # Arrange the elements\n",
    "        controls = widgets.HBox([play_btn, gen_btn, clear_btn])\n",
    "        main_layout = widgets.VBox([grid, controls, self.output])\n",
    "        display(main_layout)\n",
    "\n",
    "        display(self.output)\n",
    "    \n",
    "    def _make_midi_from_widgets(self):\n",
    "        track = symusic.Track(is_drum=True)\n",
    "\n",
    "        tpq = self.encoder.tpq\n",
    "        subdiv = self.encoder.subdivision\n",
    "\n",
    "        steps = { i: [] for i in range(self.num_steps)}\n",
    "        for drum, boxes in self.drum_checkboxes.items():\n",
    "            for i, box in enumerate(boxes):\n",
    "                if box.value:\n",
    "                    track.notes.append(symusic.Note(int(i * tpq * 4 / subdiv), 80, drum_hits[drum], 80))\n",
    "        \n",
    "        track.sort()\n",
    "        midi = symusic.Score(tpq)\n",
    "        midi.tracks.append(track)\n",
    "        return midi\n",
    "        \n",
    "    def _play_pattern(self, _):\n",
    "        midi = self._make_midi_from_widgets()\n",
    "        with self.output:\n",
    "            ipd.clear_output()\n",
    "            display(utils.midi_to_audio_display(self.fs, midi))\n",
    "\n",
    "    def _generate_pattern(self, _):\n",
    "        midi = self._make_midi_from_widgets()\n",
    "        # sequence = sequence.where(sequence == self.tokeniser['<rest>'], torch.full_like(sequence, self.tokeniser['<mask>']))\n",
    "        with torch.no_grad():\n",
    "            self.model.eval()\n",
    "\n",
    "            hits, pitch, _ = self.encoder.encode(midi)\n",
    "            sequence = torch.cat((hits, pitch), dim=-1).unsqueeze(0).to(device)\n",
    "            hit_logits, pitch_logits, vels = self.model(sequence)\n",
    "\n",
    "            hit_probs = (torch.sigmoid(hit_logits) > 0.2).to(torch.float32)\n",
    "            pitch_probs = (torch.sigmoid(pitch_logits) > 0.3).to(torch.float32)\n",
    "\n",
    "            midi = self.encoder.decode((hit_probs.squeeze(0), pitch_probs.squeeze(0), vels.squeeze(0)))\n",
    "            # pred_sequence = torch.multinomial(torch.softmax(logits / 0.1, dim=-1), num_samples=1) # TODO add temperature\n",
    "            # sequence = sequence.masked_scatter(sequence == self.tokeniser['<mask>'], pred_sequence)\n",
    "        \n",
    "        with self.output:\n",
    "            ipd.clear_output()\n",
    "            display(utils.midi_to_audio_display(self.fs, midi))\n",
    "\n",
    "\n",
    "    def _clear_pattern(self, _):\n",
    "        for boxes in self.drum_checkboxes.values():\n",
    "            for box in boxes:\n",
    "                box.value = False\n",
    "        \n",
    "DrumSequencer(model, drum_encoder, FS)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
