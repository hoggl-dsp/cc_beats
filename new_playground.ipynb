{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e668adaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "IS_COLAB = is_colab()\n",
    "\n",
    "if IS_COLAB:\n",
    "  !pip install git+https://github.com/hoggl-dsp/cc_beats\n",
    "\n",
    "  !apt-get install fluidsynth\n",
    "  !mkdir data\n",
    "  !mkdir data/sf2\n",
    "  !wget -O data/sf2/Xpand_2_-_Practice_Room_Kit.sf2 'https://musical-artifacts.com/artifacts/6296/Xpand_2_-_Practice_Room_Kit.sf2'\n",
    "\n",
    "  !wget -O data/groove_midi_only.zip 'https://storage.googleapis.com/magentadata/datasets/groove/groove-v1.0.0-midionly.zip'\n",
    "  !unzip data/groove_midi_only.zip -d data/\n",
    "\n",
    "  !wget -O data/clean_midi.tar.gz 'http://hog.ee.columbia.edu/craffel/lmd/clean_midi.tar.gz'\n",
    "  !tar -xf data/clean_midi.tar.gz -C data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8fbf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import symusic\n",
    "import symusic.types\n",
    "from midi2audio import FluidSynth\n",
    "\n",
    "import IPython.display as ipd\n",
    "import tqdm\n",
    "import os\n",
    "\n",
    "from typing import Callable\n",
    "\n",
    "from cc_beats import tokeniser, modules, utils\n",
    "\n",
    "sf_path = 'data/sf2/Xpand_2_-_Practice_Room_Kit.sf2'\n",
    "FS = FluidSynth(sf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e76589",
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_files = []\n",
    "for root, _, files in os.walk(os.path.join('data', 'groove')):\n",
    "    midi_files.extend([os.path.join(root, file) for file in files if file.endswith('.mid') and 'beat' in file and 'eval' not in os.path.basename(root)])\n",
    "\n",
    "midi_files = []\n",
    "for root, _, files in os.walk(os.path.join('data', 'clean_midi')):\n",
    "    midi_files.extend([os.path.join(root, file) for file in files if file.endswith('.mid')]) # and 'beat' in file and 'eval' not in os.path.basename(root)])\n",
    "\n",
    "for i in range(20):\n",
    "    print(midi_files[i])\n",
    "\n",
    "midi_file = symusic.Score(midi_files[0])\n",
    "\n",
    "encoder = tokeniser.DrumSequenceEncoder(subdivision=16)\n",
    "\n",
    "tok_sequence = encoder.encode(midi_file)\n",
    "returned_midi = encoder.decode(tok_sequence)\n",
    "\n",
    "returned_midi.time_signatures.extend(midi_file.time_signatures)\n",
    "returned_midi.key_signatures.extend(midi_file.key_signatures)\n",
    "returned_midi.tempos.extend(midi_file.tempos)\n",
    "\n",
    "print(\"Actual File\")\n",
    "print(midi_file)\n",
    "print(midi_file.tracks[0].notes[:10])\n",
    "print('-----------------------------')\n",
    "\n",
    "print(\"Decoded File\")\n",
    "print(returned_midi)\n",
    "print(returned_midi.tracks[0].notes[:10])\n",
    "print('-----------------------------')\n",
    "\n",
    "display(utils.midi_to_audio_display(FS, midi_file))\n",
    "display(utils.midi_to_audio_display(FS, returned_midi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e020893b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_midi_data_files(root_dir: str, filter_fn: Callable[[str, str], bool] = lambda root, file: True):\n",
    "    midi_files = []\n",
    "    for root, _, files in os.walk(root_dir):\n",
    "        midi_files.extend([os.path.join(root, file) for file in files if filter_fn(root, file)])\n",
    "    return midi_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ebb235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lakh clean_set analysis\n",
    "import pandas as pd\n",
    "\n",
    "def do_lakh_analysis():\n",
    "    files = get_midi_data_files(os.path.join('data', 'clean_midi'))\n",
    "\n",
    "    data = {\n",
    "        'File': [],\n",
    "        'File Length (quarters)': [],\n",
    "        'Tempos': [],\n",
    "        'Is 4/4': [],\n",
    "        'Num Drum Tracks': [],\n",
    "        'Drum Hit Count': [],\n",
    "        'Unique Drum Hits': [],\n",
    "        'First Drum (quarters)': [],\n",
    "        'Last Drum (quarters)': [],\n",
    "    }\n",
    "    for file in tqdm.tqdm(files, desc='Analysing Midi Files'):\n",
    "        midi = None\n",
    "        drum_tracks = None\n",
    "        data['File'].append(file)\n",
    "        try:\n",
    "            midi = symusic.Score.from_file(file)\n",
    "            data['File Length (quarters)'].append((midi.end() - midi.start()) / midi.tpq)\n",
    "            data['Tempos'].append(midi.tempos)\n",
    "            data['Is 4/4'].append(len(midi.time_signatures) == 1 and (midi.time_signatures[0].numerator == 4 and midi.time_signatures[0].denominator == 4))\n",
    "\n",
    "            drum_tracks = [track for track in midi.tracks if track.is_drum]\n",
    "            data['Num Drum Tracks'].append(len(drum_tracks))\n",
    "\n",
    "            count = 0\n",
    "            note_set = set()\n",
    "            first_note_time = None\n",
    "            last_note_time = None\n",
    "            for track in drum_tracks:\n",
    "                count += len(track.notes)\n",
    "                for note in track.notes:\n",
    "                    note_set.add(note.pitch)\n",
    "                    if first_note_time is None or first_note_time > note.time:\n",
    "                        first_note_time = note.time\n",
    "                    if last_note_time is None or last_note_time < note.time:\n",
    "                        last_note_time = note.time\n",
    "            \n",
    "            data['Drum Hit Count'].append(count)\n",
    "            data['Unique Drum Hits'].append(note_set)\n",
    "            data['First Drum (quarters)'].append(first_note_time / midi.tpq if first_note_time is not None else None)\n",
    "            data['Last Drum (quarters)'].append(last_note_time / midi.tpq if last_note_time is not None else None)\n",
    "\n",
    "        except:\n",
    "            for key in data:\n",
    "                if key != 'File':\n",
    "                    data[key].append(None)\n",
    "    \n",
    "\n",
    "    \n",
    "    lakh_df = pd.DataFrame(data)\n",
    "\n",
    "    print(\" ***** Filtering Dataset *****\")\n",
    "    print(\"Full Dataset:\", len(lakh_df))\n",
    "    lakh_df = lakh_df[~lakh_df.isna()]\n",
    "    print(\"Without unreadable files:\", len(lakh_df))\n",
    "    lakh_df = lakh_df[lakh_df['Num Drum Tracks'] > 0]\n",
    "    print(\"Without files with no drums:\", len(lakh_df))\n",
    "    lakh_df = lakh_df[lakh_df['Is 4/4']]\n",
    "    print(\"Without files not in 4/4:\", len(lakh_df))\n",
    "    lakh_df = lakh_df[lakh_df['Drum Hit Count'] > 256]\n",
    "    print(\"Without files with less than 256 drum hits\", len(lakh_df))\n",
    "    lakh_df = lakh_df[lakh_df['Drum Hit Count'] / (lakh_df['File Length (quarters)']) >= 1.0]\n",
    "    print(\"Without files with note density < 1 per quarter:\", len(lakh_df))\n",
    "    lakh_df = lakh_df[lakh_df['File Length (quarters)'] < 1000.0]\n",
    "    print(\"Without files with > 1000 beats:\", len(lakh_df))\n",
    "\n",
    "    return lakh_df\n",
    "    \n",
    "lakh_clean_df = do_lakh_analysis()\n",
    "\n",
    "print(lakh_clean_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1676ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrumInpaintingTransformer(torch.nn.Module):\n",
    "    def __init__(self, num_pitches: int, embedding_dim: int = 64, num_layers: int = 6, pitch_pos_weight: float = 2.0, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.input_proj = torch.nn.Linear(1 + 2 * num_pitches, embedding_dim)\n",
    "\n",
    "        self.transformer = torch.nn.ModuleList([\n",
    "            modules.TransformerLayerWithRelativeAttention(embedding_dim, num_heads=8, dropout=0.1)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.output_hit_proj = torch.nn.Linear(embedding_dim, 1)\n",
    "        self.output_pitches_proj = torch.nn.Linear(embedding_dim, num_pitches)\n",
    "        self.output_velocities_proj = torch.nn.Linear(embedding_dim, num_pitches)\n",
    "\n",
    "        self.hit_loss = torch.nn.BCEWithLogitsLoss()\n",
    "        if isinstance(pitch_pos_weight, float):\n",
    "            pitch_pos_weight = torch.tensor(pitch_pos_weight)\n",
    "        self.pitch_loss = torch.nn.BCEWithLogitsLoss(pos_weight=pitch_pos_weight)\n",
    "        self.vel_loss = torch.nn.MSELoss()\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, attention_mask=None):\n",
    "        # Input dims [batch, seq_len, 1 + 2 * num_pitches]\n",
    "        \n",
    "        x = self.input_proj(x) # [batch, seq_len, embedding_dim]\n",
    "\n",
    "        for layer in self.transformer:\n",
    "            x = layer(x) # [batch, seq_len, embedding_dim]\n",
    "        \n",
    "        hit_logits = self.output_hit_proj(x)\n",
    "        pitch_logits = self.output_pitches_proj(x)\n",
    "        vel_logits = torch.sigmoid(self.output_velocities_proj(x))\n",
    "\n",
    "        return hit_logits, pitch_logits, vel_logits\n",
    "    \n",
    "    def loss_function(self, \n",
    "            preds: tuple[torch.Tensor, torch.Tensor, torch.Tensor], \n",
    "            truths: tuple[torch.Tensor, torch.Tensor, torch.Tensor],\n",
    "            inpainting_mask: torch.Tensor\n",
    "        ) -> torch.Tensor:\n",
    "        pred_hits, pred_pitches, pred_vels = preds\n",
    "        true_hits, true_pitches, true_vels = truths\n",
    "        \n",
    "        device = pred_hits.device\n",
    "\n",
    "        hit_loss = self.hit_loss(pred_hits[inpainting_mask], true_hits[inpainting_mask])\n",
    "\n",
    "        \n",
    "        hit_mask = inpainting_mask & (true_hits > 0.5)\n",
    "        hit_mask = hit_mask.expand(-1, -1, 9)\n",
    "\n",
    "        pitch_loss = self.pitch_loss(pred_pitches[hit_mask], true_pitches[hit_mask]) if hit_mask.any() else torch.tensor(0.0, device=device)\n",
    "\n",
    "        hit_pitch_mask = hit_mask & (true_pitches > 0.5)\n",
    "        vel_loss = self.vel_loss(pred_vels[hit_pitch_mask], true_vels[hit_pitch_mask]) if hit_pitch_mask.any() else torch.tensor(0.0, device=device)\n",
    "\n",
    "        return hit_loss + pitch_loss + vel_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869a25ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "def make_dataset(encoder: tokeniser.DrumSequenceEncoder, midi_files: list[str], max_seq_length: int = 64):\n",
    "    sequences = encoder.encode_all(midi_files)\n",
    "    split_hits = []\n",
    "    split_pithes = []\n",
    "    split_velocities = []\n",
    "    for hits, pitches, velocities in sequences:\n",
    "        len_padding = max_seq_length - (hits.size(0) % max_seq_length)\n",
    "        if len_padding > 0:\n",
    "            hits = torch.cat([hits, torch.zeros(len_padding, hits.size(1), dtype=hits.dtype, device=hits.device)])\n",
    "            pitches = torch.cat([pitches, torch.zeros(len_padding, pitches.size(1), dtype=pitches.dtype, device=pitches.device)])\n",
    "            velocities = torch.cat([velocities, torch.zeros(len_padding, velocities.size(1), dtype=velocities.dtype, device=velocities.device)])\n",
    "        \n",
    "        split_hits.extend(torch.tensor_split(hits, int(hits.size(0) / max_seq_length)))\n",
    "        split_pithes.extend(torch.tensor_split(pitches, int(pitches.size(0) / max_seq_length)))\n",
    "        split_velocities.extend(torch.tensor_split(velocities, int(velocities.size(0) / max_seq_length)))\n",
    "    \n",
    "    return torch.utils.data.TensorDataset(\n",
    "        torch.stack(split_hits, dim=0),\n",
    "        torch.stack(split_pithes, dim=0),\n",
    "        torch.stack(split_velocities, dim=0)\n",
    "    )\n",
    "\n",
    "drum_encoder = tokeniser.DrumSequenceEncoder(subdivision=16)\n",
    "dataset = make_dataset(drum_encoder, lakh_clean_df['File'].to_list(), max_seq_length=128)\n",
    "\n",
    "print(\"Dataset_Size:\", len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd22dc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training parameters\n",
    "num_epochs = 1\n",
    "batch_size = 32\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "model = DrumInpaintingTransformer(num_pitches=9, embedding_dim=16, num_layers=16, pitch_pos_weight=1.5)\n",
    "model = model.to(device)  # Move model to the appropriate device\n",
    "\n",
    "# Create a DataLoader for batching\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_set, valid_set = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=2e-5, weight_decay=1e-6)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    valid_set, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for j, (hits, pitches, vels) in enumerate(train_loader, 1):\n",
    "        optimiser.zero_grad()\n",
    "        \n",
    "        hits, pitches, vels = hits.to(device), pitches.to(device), vels.to(device)\n",
    "        masked_hits, masked_pitches, masked_vels = hits.clone(), pitches.clone(), vels.clone()\n",
    "\n",
    "        mask = torch.rand_like(hits, device=device) < 0.25\n",
    "        masked_hits[mask] = 0.5\n",
    "        masked_pitches[mask.squeeze(), :] = 0.0\n",
    "        masked_vels[mask.squeeze(), :] = 0.0\n",
    "        \n",
    "        model_input = torch.cat((masked_hits, masked_pitches, masked_vels), dim=-1)\n",
    "\n",
    "        pred_hits, pred_pitches, pred_vels = model(model_input)\n",
    "        loss = model.loss_function((pred_hits, pred_pitches, pred_vels), (hits, pitches, vels), mask)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        if j % 10 == 0:\n",
    "            print(f'[{j}/{len(train_loader)}] - Running loss = {total_train_loss / len(train_loader)}')\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        print(loss)\n",
    "    \n",
    "    # Keep track of training loss\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for hits, pitches, vels in val_loader:\n",
    "            hits, pitches, vels = hits.to(device), pitches.to(device), vels.to(device)\n",
    "            \n",
    "            # Create same masking pattern as in training\n",
    "            masked_hits, masked_pitches, masked_vels = hits.clone(), pitches.clone(), vels.clone()\n",
    "            mask = torch.rand_like(hits, device=device) < 0.25\n",
    "            masked_hits[mask] = 0.5\n",
    "            masked_pitches[mask.squeeze(), :] = 0.0\n",
    "            masked_vels[mask.squeeze(), :] = 0.0\n",
    "            \n",
    "            model_input = torch.cat((masked_hits, masked_pitches, masked_vels), dim=-1)\n",
    "            \n",
    "            pred_hits, pred_pitches, pred_vels = model(model_input)\n",
    "            loss = model.loss_function((pred_hits, pred_pitches, pred_vels), (hits, pitches, vels), mask)\n",
    "            \n",
    "            total_val_loss += loss.item()\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    \n",
    "    print(f\"Epoch {i+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, 'b-', label='Training Loss')\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, 'r-', label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Save the model to disk\n",
    "torch.save(model.state_dict(), 'drum_inpainting_model.pth')\n",
    "print(\"Model saved to 'drum_inpainting_model.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baf56ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(length: int, prompt_hits: list[tuple[int, int, float]]):\n",
    "    hits = torch.zeros(1, length, 1)\n",
    "    pitches = torch.zeros(1, length, 9)\n",
    "    velocities = torch.zeros(1, length, 9)\n",
    "\n",
    "    for index, pitch, velocity in prompt_hits:\n",
    "        hits[:, index, :] = 1.0\n",
    "        pitches[:, index, pitch] = 1.0\n",
    "        velocities[:, index, pitch] = velocity\n",
    "\n",
    "    return hits.to(device), pitches.to(device), velocities.to(device)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else \"cpu\")\n",
    "# model = DrumInpaintingTransformer(num_pitches=9, num_layers=4, pitch_pos_weight=5.0)\n",
    "model.load_state_dict(torch.load('drum_inpainting_model.pth', map_location=device))\n",
    "model = model.to(device)\n",
    "prompt = [\n",
    "    (0, drum_encoder.pitch_to_index[36], 0.8),\n",
    "    (3, drum_encoder.pitch_to_index[36], 0.4),\n",
    "    (4, drum_encoder.pitch_to_index[38], 0.8),\n",
    "    (6, drum_encoder.pitch_to_index[36], 0.7),\n",
    "    (11, drum_encoder.pitch_to_index[36], 0.5),\n",
    "    (12, drum_encoder.pitch_to_index[38], 0.8),\n",
    "    (14, drum_encoder.pitch_to_index[36], 0.7),\n",
    "    (19, drum_encoder.pitch_to_index[36], 0.5),\n",
    "    (20, drum_encoder.pitch_to_index[38], 0.8),\n",
    "    (22, drum_encoder.pitch_to_index[36], 0.7),\n",
    "    (27, drum_encoder.pitch_to_index[36], 0.5),\n",
    "    (28, drum_encoder.pitch_to_index[38], 0.8),\n",
    "    (29, drum_encoder.pitch_to_index[38], 0.3),\n",
    "]\n",
    "prompt_hits, prompt_pitches, prompt_vels = make_prompt(32, prompt)\n",
    "midi = encoder.decode((prompt_hits.squeeze(), prompt_pitches.squeeze(), prompt_vels.squeeze()))\n",
    "ipd.display(utils.midi_to_audio_display(FS, midi))\n",
    "\n",
    "hit_logits, pitch_logits, vels = model(torch.cat((prompt_hits, prompt_pitches, prompt_vels), dim=-1))\n",
    "hit_probs = torch.sigmoid(hit_logits)\n",
    "pitch_probs = torch.sigmoid(pitch_logits)\n",
    "\n",
    "hits = (torch.sigmoid(hit_logits) > 0.3).to(dtype=prompt_hits.dtype)\n",
    "pitches = (torch.sigmoid(pitch_logits) > 0.5).to(dtype=prompt_pitches.dtype)\n",
    "\n",
    "mask = prompt_hits > 0.5\n",
    "merged_hits = prompt_hits.masked_scatter(mask, hits)\n",
    "merged_pitches = prompt_pitches.masked_scatter(mask.expand_as(prompt_pitches), pitches)\n",
    "merged_vels = prompt_vels.masked_scatter(mask.expand_as(prompt_vels), vels)\n",
    "\n",
    "print(merged_hits)\n",
    "print(merged_pitches)\n",
    "midi = encoder.decode((merged_hits.squeeze(), merged_pitches.squeeze(), merged_vels.squeeze()))\n",
    "display(utils.midi_to_audio_display(FS, midi))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
