{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "def is_colab():\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False"
      ],
      "metadata": {
        "id": "6OuEo1HFxyaO"
      },
      "id": "6OuEo1HFxyaO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if is_colab():\n",
        "  !pip install git+https://github.com/hoggl-dsp/cc_beats\n",
        "\n",
        "  !apt-get install fluidsynth\n",
        "  !mkdir data\n",
        "  !mkdir data/sf2\n",
        "  !wget -O data/sf2/Xpand_2_-_Practice_Room_Kit.sf2 'https://musical-artifacts.com/artifacts/6296/Xpand_2_-_Practice_Room_Kit.sf2'\n",
        "  !wget -O data/groove_midi_only.zip 'https://storage.googleapis.com/magentadata/datasets/groove/groove-v1.0.0-midionly.zip'\n",
        "  !unzip data/groove_midi_only.zip -d data/"
      ],
      "metadata": {
        "id": "j69t8-tGyMuB"
      },
      "id": "j69t8-tGyMuB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "241d7450",
      "metadata": {
        "id": "241d7450"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import symusic\n",
        "import symusic.types\n",
        "from midi2audio import FluidSynth\n",
        "\n",
        "import IPython.display as ipd\n",
        "\n",
        "import os\n",
        "\n",
        "from cc_beats import modules, tokeniser, utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c42234a",
      "metadata": {
        "id": "7c42234a"
      },
      "outputs": [],
      "source": [
        "midi_file = symusic.Score.from_file('data/groove/drummer1/session1/1_funk_80_beat_4-4.mid')\n",
        "print(midi_file)\n",
        "\n",
        "# sf_path = '/opt/homebrew/Cellar/fluid-synth/2.4.5/share/fluid-synth/sf2/VintageDreamsWaves-v2.sf2'\n",
        "sf_path = 'data/sf2/Xpand_2_-_Practice_Room_Kit.sf2'\n",
        "\n",
        "fs = FluidSynth(sf_path)\n",
        "\n",
        "# synth = Synthesizer(sf_path)\n",
        "\n",
        "\n",
        "# fs.play_midi('data/groove_midi_only/drummer1/session1/1_funk_80_beat_4-4.mid')\n",
        "\n",
        "utils.midi_to_audio_display(fs, midi_file)\n",
        "print(midi_file)\n",
        "print(midi_file.tracks[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "893a395a",
      "metadata": {
        "id": "893a395a"
      },
      "outputs": [],
      "source": [
        "midi_files = []\n",
        "for root, _, files in os.walk(os.path.join('data', 'groove')):\n",
        "    midi_files.extend([os.path.join(root, file) for file in files if file.endswith('.mid') and 'beat' in file and 'eval' not in os.path.basename(root)])\n",
        "\n",
        "for i in range(20):\n",
        "    print(midi_files[i])\n",
        "\n",
        "midi_file = symusic.Score(midi_files[0])\n",
        "\n",
        "drum_tokeniser = tokeniser.DrumSequenceTokeniser(subdivision=16, velocity_bands=4)\n",
        "\n",
        "tok_sequence = drum_tokeniser.encode(midi_file)\n",
        "returned_midi = drum_tokeniser.decode(tok_sequence)\n",
        "\n",
        "returned_midi.time_signatures.extend(midi_file.time_signatures)\n",
        "returned_midi.key_signatures.extend(midi_file.key_signatures)\n",
        "returned_midi.tempos.extend(midi_file.tempos)\n",
        "\n",
        "print(\"Actual File\")\n",
        "print(midi_file)\n",
        "print(midi_file.tracks[0].notes[:10])\n",
        "print('-----------------------------')\n",
        "\n",
        "print(\"Decoded File\")\n",
        "print(returned_midi)\n",
        "print(returned_midi.tracks[0].notes[:10])\n",
        "print('-----------------------------')\n",
        "\n",
        "display(utils.midi_to_audio_display(fs, midi_file))\n",
        "display(utils.midi_to_audio_display(fs, returned_midi))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "724baa04",
      "metadata": {
        "id": "724baa04"
      },
      "outputs": [],
      "source": [
        "class DrumInpaintingTransformer(torch.nn.Module):\n",
        "    def __init__(self, vocab_size: int, embedding_dim: int = 256, num_layers: int = 8, num_heads: int = 8, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        self.transformer = torch.nn.ModuleList([\n",
        "            modules.TransformerLayerWithRelativeAttention(embedding_dim, num_heads=num_heads, dropout=0.1)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.dense_out = torch.nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x, attention_mask=None):\n",
        "        # Embed tokens\n",
        "        x = self.embedding(x)  # [batch_size, seq_len, embedding_dim]\n",
        "\n",
        "        for layer in self.transformer:\n",
        "            x = layer(x, attention_mask)\n",
        "\n",
        "        # Project to vocabulary\n",
        "        output = self.dense_out(x)  # [batch_size, seq_len, vocab_size]\n",
        "\n",
        "        return output\n",
        "\n",
        "# class DrumInpaintingTransformer(torch.nn.Module):\n",
        "#     def __init__(self, vocab_size: int, embedding_dim: int = 512, *args, **kwargs):\n",
        "#         super().__init__(*args, **kwargs)\n",
        "\n",
        "#         self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "#         encoder_layer = torch.nn.TransformerEncoderLayer(\n",
        "#             d_model=embedding_dim,\n",
        "#             nhead=16,\n",
        "#             dim_feedforward=512,\n",
        "#             batch_first=True,\n",
        "#         )\n",
        "\n",
        "#         self.transformer_encoder = torch.nn.TransformerEncoder(\n",
        "#             encoder_layer=encoder_layer,\n",
        "#             num_layers=6\n",
        "#         )\n",
        "\n",
        "#         self.dense_out = torch.nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "#     def forward(self, x, attention_mask=None):\n",
        "#         # Embed tokens\n",
        "#         x = self.embedding(x)  # [batch_size, seq_len, d_model]\n",
        "\n",
        "#         # Pass through transformer (attention_mask needs to be properly formatted for TransformerEncoder)\n",
        "#         if attention_mask is not None:\n",
        "#             # Create a mask for padding tokens (1 means masked/ignored position)\n",
        "#             padding_mask = (attention_mask == 0)\n",
        "#             x = self.transformer_encoder(x, src_key_padding_mask=padding_mask)\n",
        "#         else:\n",
        "#             x = self.transformer_encoder(x)\n",
        "\n",
        "#         # Project to vocabulary\n",
        "#         output = self.dense_out(x)  # [batch_size, seq_len, vocab_size]\n",
        "\n",
        "#         return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75641252",
      "metadata": {
        "id": "75641252"
      },
      "outputs": [],
      "source": [
        "def create_dataset(tokeniser: tokeniser.DrumSequenceTokeniser, midi_files: list[str], chunk_length: int = 64):\n",
        "    encoded_sequences = tokeniser.encode_all(midi_files)\n",
        "    seq_chunks = []\n",
        "    for seq in encoded_sequences:\n",
        "        iterated_seq = seq.copy()\n",
        "        while len(iterated_seq) > 0:\n",
        "            if len(iterated_seq) >= chunk_length:\n",
        "                seq_chunks.append(torch.tensor(iterated_seq[:chunk_length]))\n",
        "                iterated_seq = iterated_seq[chunk_length:]\n",
        "            else:\n",
        "                num_pads = chunk_length - len(iterated_seq)\n",
        "                seq_chunks.append(torch.tensor(iterated_seq + [tokeniser['<pad>'] for _ in range(num_pads)]))\n",
        "                iterated_seq = []\n",
        "    seq_chunks = torch.stack(seq_chunks, dim=0)\n",
        "    token_counts = seq_chunks.unique(return_counts=True)\n",
        "    for tok, count in zip(token_counts[0], token_counts[1]):\n",
        "        print(tok.item(), count.item())\n",
        "    return torch.utils.data.TensorDataset(seq_chunks)\n",
        "\n",
        "dataset = create_dataset(drum_tokeniser, midi_files)\n",
        "print(\"Num Chunks:\", len(dataset))\n",
        "print(\"Vocab Size:\", len(drum_tokeniser))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "483fc736",
      "metadata": {
        "id": "483fc736"
      },
      "outputs": [],
      "source": [
        "num_epochs = 30\n",
        "batch_size = 32\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else \"cpu\")\n",
        "\n",
        "model = DrumInpaintingTransformer(len(drum_tokeniser.vocab))\n",
        "model = model.to(device)  # Move model to the appropriate device\n",
        "\n",
        "# Create a DataLoader for batching\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_set, valid_set = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimiser = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_set,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    valid_set,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Training loop\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    train_batches = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        inputs = batch[0].to(device)\n",
        "        targets = inputs.clone()\n",
        "\n",
        "        # Create masked input for training (25% masking probability)\n",
        "        mask = torch.rand(inputs.shape, device=device) > 0.25\n",
        "        masked_inputs = inputs.clone()\n",
        "        masked_inputs[~mask] = drum_tokeniser['<mask>']\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimiser.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(masked_inputs)\n",
        "\n",
        "        # Reshape for loss calculation\n",
        "        batch_size, seq_len, vocab_size = outputs.shape\n",
        "        outputs_flat = outputs.reshape(-1, vocab_size)\n",
        "        targets_flat = targets.reshape(-1)\n",
        "        mask_flat = mask.reshape(-1)\n",
        "\n",
        "        # Calculate loss only on masked positions\n",
        "        loss = loss_fn(outputs_flat[~mask_flat], targets_flat[~mask_flat])\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimiser.step()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "        train_batches += 1\n",
        "\n",
        "    avg_train_loss = total_train_loss / max(1, train_batches)\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    val_batches = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            inputs = batch[0].to(device)\n",
        "            targets = inputs.clone()\n",
        "\n",
        "            # Create masked input for validation\n",
        "            mask = torch.rand(inputs.shape, device=device) > 0.25\n",
        "            masked_inputs = inputs.clone()\n",
        "            masked_inputs[~mask] = drum_tokeniser['<mask>']\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(masked_inputs)\n",
        "\n",
        "            # Reshape for loss calculation\n",
        "            batch_size, seq_len, vocab_size = outputs.shape\n",
        "            outputs_flat = outputs.reshape(-1, vocab_size)\n",
        "            targets_flat = targets.reshape(-1)\n",
        "            mask_flat = mask.reshape(-1)\n",
        "\n",
        "            # Calculate loss only on masked positions\n",
        "            loss = loss_fn(outputs_flat[~mask_flat], targets_flat[~mask_flat])\n",
        "\n",
        "            total_val_loss += loss.item()\n",
        "            val_batches += 1\n",
        "\n",
        "    avg_val_loss = total_val_loss / max(1, val_batches)\n",
        "    val_losses.append(avg_val_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "# Plot training and validation loss\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c41ab3b9",
      "metadata": {
        "id": "c41ab3b9"
      },
      "outputs": [],
      "source": [
        "torch.save(model, 'model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14269a85",
      "metadata": {
        "id": "14269a85"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "batch_size = 32\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else \"cpu\")\n",
        "\n",
        "model = torch.load('model.pth', map_location=device, weights_only=False)\n",
        "# model.load_state_dict()\n",
        "model = model.to(device)  # Move model to the appropriate device\n",
        "\n",
        "input_sequence = random.choice(dataset)[0].unsqueeze(0).to(device)\n",
        "\n",
        "model.eval()\n",
        "# Create masked input for training (20% masking probability)\n",
        "mask = torch.rand(input_sequence.shape, device=device) > 0.25\n",
        "masked_inputs = input_sequence.clone()\n",
        "masked_inputs[~mask] = drum_tokeniser['<mask>']\n",
        "\n",
        "output_logits = model(masked_inputs).squeeze()\n",
        "print(output_logits.shape)\n",
        "output_preds = torch.multinomial(torch.softmax(output_logits, dim=-1) / 0.5, num_samples=1)\n",
        "\n",
        "print(output_preds)\n",
        "output_sequence = input_sequence.masked_scatter(~mask, output_preds)\n",
        "\n",
        "input_midi = drum_tokeniser.decode(input_sequence.squeeze())\n",
        "output_midi = drum_tokeniser.decode(output_sequence.squeeze())\n",
        "\n",
        "for in_note, out_note in zip(input_midi.tracks[0].notes, output_midi.tracks[0].notes):\n",
        "    print(in_note, ' - ', out_note)\n",
        "\n",
        "display(utils.midi_to_audio_display(fs, input_midi))\n",
        "display(utils.midi_to_audio_display(fs, output_midi))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bb0c58b",
      "metadata": {
        "id": "8bb0c58b"
      },
      "outputs": [],
      "source": [
        "sequence = [\n",
        "    'p36_v2', '<mask>', '<mask>', 'p36_v1', 'p38_v3', '<mask>', 'p36_v1', '<mask>', '<mask>', '<mask>', '<mask>', 'p36_v1', 'p38_v3', '<mask>', 'p36_v1', '<mask>',\n",
        "    '<mask>', '<mask>', '<mask>', 'p36_v1', 'p38_v3', '<mask>', 'p36_v1', '<mask>', '<mask>', '<mask>', '<mask>', 'p36_v1', 'p38_v3', '<mask>', '<mask>', '<mask>'\n",
        "]\n",
        "tok_sequence = torch.tensor([drum_tokeniser[tok] for tok in sequence], device=device)\n",
        "\n",
        "output_logits = model(tok_sequence.unsqueeze(0)).squeeze()\n",
        "\n",
        "mask = tok_sequence == drum_tokeniser['<mask>']\n",
        "rests = torch.full_like(tok_sequence, drum_tokeniser['<rest>'])\n",
        "\n",
        "output_preds = torch.multinomial(torch.softmax(output_logits, dim=-1) / 0.1, num_samples=1).squeeze()\n",
        "print(torch.topk(torch.softmax(output_logits / 0.1, dim=-1), 10, dim=-1))\n",
        "# output_preds = torch.argmax(output_logits, dim=-1).squeeze()\n",
        "\n",
        "template_midi = drum_tokeniser.decode(tok_sequence.masked_scatter(mask, rests))\n",
        "predicted_midi = drum_tokeniser.decode(output_preds)\n",
        "output_midi = drum_tokeniser.decode(tok_sequence.masked_scatter(mask, output_preds))\n",
        "\n",
        "display(utils.midi_to_audio_display(fs, template_midi))\n",
        "display(utils.midi_to_audio_display(fs, predicted_midi))\n",
        "display(utils.midi_to_audio_display(fs, output_midi))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}