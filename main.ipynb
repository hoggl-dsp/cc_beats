{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111661d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "    \n",
    "IS_COLAB = is_colab()\n",
    "\n",
    "if IS_COLAB:\n",
    "  !pip install git+https://github.com/hoggl-dsp/cc_beats\n",
    "\n",
    "  !apt-get install fluidsynth\n",
    "  !mkdir data\n",
    "  !mkdir data/sf2\n",
    "  !wget -O data/sf2/Xpand_2_-_Practice_Room_Kit.sf2 'https://musical-artifacts.com/artifacts/6296/Xpand_2_-_Practice_Room_Kit.sf2'\n",
    "  \n",
    "  !wget -O data/groove_midi_only.zip 'https://storage.googleapis.com/magentadata/datasets/groove/groove-v1.0.0-midionly.zip'\n",
    "  !unzip data/groove_midi_only.zip -d data/\n",
    "\n",
    "  !wget -O data/clean_midi.tar.gz 'http://hog.ee.columbia.edu/craffel/lmd/clean_midi.tar.gz'\n",
    "  !tar -xf data/clean_midi.tar.gz -C data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab129ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import symusic\n",
    "import symusic.types\n",
    "from midi2audio import FluidSynth\n",
    "\n",
    "import IPython.display as ipd\n",
    "\n",
    "import os\n",
    "import random\n",
    "import tqdm\n",
    "from typing import Callable\n",
    "\n",
    "from cc_beats import modules, tokeniser, utils\n",
    "\n",
    "sf_path = 'data/sf2/Xpand_2_-_Practice_Room_Kit.sf2'\n",
    "FS = FluidSynth(sf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb30ba53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lakh_midi_files = []\n",
    "# for root, _, files in os.walk(os.path.join('data', 'lmd_full')):\n",
    "#     lakh_midi_files.extend([os.path.join(root, file) for file in files if file.endswith('.mid')])\n",
    "\n",
    "\n",
    "# lakh_midi_with_drums = []\n",
    "# for file in random.sample(lakh_midi_files, 10):\n",
    "#     print(\" ---------------------------------------------------------------------------- \")\n",
    "#     print(\"Loading:\", file)\n",
    "#     try:\n",
    "#         midi = symusic.Score(file)\n",
    "#         midi.tracks = [track for track in midi.tracks if track.is_drum]\n",
    "#         num_drum_tracks = len(midi.tracks)\n",
    "#         print(f\"Found {num_drum_tracks} drum tracks\")\n",
    "#         if num_drum_tracks > 0:\n",
    "#             lakh_midi_with_drums.append(midi)\n",
    "#     except:\n",
    "#         print(\"Loading failed, skipping...\")\n",
    "#         continue\n",
    "\n",
    "# drum_tokeniser = tokeniser.DrumSequenceTokeniser(subdivision=16, velocity_bands=4)\n",
    "# for midi in lakh_midi_with_drums:\n",
    "#     reconstructed_midi = drum_tokeniser.decode(drum_tokeniser.encode(midi))\n",
    "#     print(\" ---------------------------------------------------------------------------- \")\n",
    "#     display(utils.midi_to_audio_display(FS, midi))\n",
    "#     display(utils.midi_to_audio_display(FS, reconstructed_midi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5129a7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_midi_data_files(root_dir: str, filter_fn: Callable[[str, str], bool] = lambda x, y: True):\n",
    "    midi_files = []\n",
    "    for root, _, files in os.walk(root_dir):\n",
    "        midi_files.extend([os.path.join(root, file) for file in files if filter_fn(root, file)])\n",
    "    return midi_files\n",
    "\n",
    "def create_dataset(tokeniser: tokeniser.DrumSequenceTokeniser, midi_files: list[str] | list[symusic.types.Score], chunk_length: int = 64):\n",
    "    midis = []\n",
    "    for file in tqdm.tqdm(midi_files, desc='Loading Midi Files'):\n",
    "        try:\n",
    "            midi = symusic.Score.from_file(file)\n",
    "            midi.tracks = [track for track in midi.tracks if track.is_drum]\n",
    "            if len(midi.tracks) > 0:\n",
    "                # print(f\"Found {len(midi.tracks)} drum tracks\")\n",
    "                midis.append(midi)\n",
    "            else:\n",
    "                # print(\"No drum tracks, skipping...\")\n",
    "                pass\n",
    "        except:\n",
    "            # print(\"Loading failed, skipping...\")\n",
    "            pass\n",
    "    \n",
    "    print(f\"Loaded {len(midis)} midi files with at least one drum track\")\n",
    "    \n",
    "    \n",
    "    encoded_sequences = []\n",
    "    for midi in tqdm.tqdm(midis, desc='Encoding Midi Files'):\n",
    "        sequence = tokeniser.encode(midi)\n",
    "        if sequence is not None:\n",
    "            encoded_sequences.append(sequence)\n",
    "\n",
    "    seq_chunks = []\n",
    "    for seq in tqdm.tqdm(encoded_sequences, desc='Chunking Midi Files'):\n",
    "        iterated_seq = seq.copy()\n",
    "        if len(iterated_seq) > 1e7:\n",
    "            continue\n",
    "        while len(iterated_seq) > 0:\n",
    "            if len(iterated_seq) >= chunk_length:\n",
    "                seq_chunks.append(torch.tensor(iterated_seq[:chunk_length]))\n",
    "                iterated_seq = iterated_seq[chunk_length:]\n",
    "            else:\n",
    "                num_pads = chunk_length - len(iterated_seq)\n",
    "                seq_chunks.append(torch.tensor(iterated_seq + [tokeniser['<pad>'] for _ in range(num_pads)]))\n",
    "                iterated_seq = []\n",
    "    seq_chunks = torch.stack(seq_chunks, dim=0)\n",
    "    token_counts = seq_chunks.unique(return_counts=True)\n",
    "    # for tok, count in zip(token_counts[0], token_counts[1]):\n",
    "    #     print(tok.item(), count.item())\n",
    "    return torch.utils.data.TensorDataset(seq_chunks)\n",
    "\n",
    "drum_tokeniser = tokeniser.DrumSequenceTokeniser(subdivision=16, velocity_bands=4)\n",
    "dataset = create_dataset(\n",
    "    drum_tokeniser,\n",
    "    get_midi_data_files(os.path.join('data', 'groove'), lambda root, file: 'beat' in file and 'eval' not in os.path.basename(root))\n",
    "      + get_midi_data_files(os.path.join('data', 'clean_midi'))\n",
    ")\n",
    "print(\"Num Chunks:\", len(dataset))\n",
    "print(\"Vocab Size:\", len(drum_tokeniser))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d0f53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrumInpaintingTransformer(torch.nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int = 256, num_layers: int = 8, num_heads: int = 8, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.transformer = torch.nn.ModuleList([\n",
    "            modules.TransformerLayerWithRelativeAttention(embedding_dim, num_heads=num_heads, dropout=0.1)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.dense_out = torch.nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        # Embed tokens\n",
    "        x = self.embedding(x)  # [batch_size, seq_len, embedding_dim]\n",
    "\n",
    "        for layer in self.transformer:\n",
    "            x = layer(x, attention_mask)\n",
    "\n",
    "        # Project to vocabulary\n",
    "        output = self.dense_out(x)  # [batch_size, seq_len, vocab_size]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bece495d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "batch_size = 32\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "model = DrumInpaintingTransformer(len(drum_tokeniser.vocab), embedding_dim=512, num_layers=16, num_heads=16)\n",
    "model = model.to(device)  # Move model to the appropriate device\n",
    "\n",
    "# Create a DataLoader for batching\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_set, valid_set = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    valid_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    train_batches = 0\n",
    "\n",
    "    for i, batch in enumerate(train_loader, 1):\n",
    "        inputs = batch[0].to(device)\n",
    "        targets = inputs.clone()\n",
    "\n",
    "        # Create masked input for training (25% masking probability)\n",
    "        mask = torch.rand(inputs.shape, device=device) > 0.25\n",
    "        masked_inputs = inputs.clone()\n",
    "        masked_inputs[~mask] = drum_tokeniser['<mask>']\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(masked_inputs)\n",
    "\n",
    "        # Reshape for loss calculation\n",
    "        batch_size, seq_len, vocab_size = outputs.shape\n",
    "        outputs_flat = outputs.reshape(-1, vocab_size)\n",
    "        targets_flat = targets.reshape(-1)\n",
    "        mask_flat = mask.reshape(-1)\n",
    "\n",
    "        # Calculate loss only on masked positions\n",
    "        loss = loss_fn(outputs_flat[~mask_flat], targets_flat[~mask_flat])\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        train_batches += 1\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f'[{i}/{len(train_loader)}] - Loss = {loss.item()}')\n",
    "\n",
    "    avg_train_loss = total_train_loss / max(1, train_batches)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    val_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs = batch[0].to(device)\n",
    "            targets = inputs.clone()\n",
    "\n",
    "            # Create masked input for validation\n",
    "            mask = torch.rand(inputs.shape, device=device) > 0.25\n",
    "            masked_inputs = inputs.clone()\n",
    "            masked_inputs[~mask] = drum_tokeniser['<mask>']\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(masked_inputs)\n",
    "\n",
    "            # Reshape for loss calculation\n",
    "            batch_size, seq_len, vocab_size = outputs.shape\n",
    "            outputs_flat = outputs.reshape(-1, vocab_size)\n",
    "            targets_flat = targets.reshape(-1)\n",
    "            mask_flat = mask.reshape(-1)\n",
    "\n",
    "            # Calculate loss only on masked positions\n",
    "            loss = loss_fn(outputs_flat[~mask_flat], targets_flat[~mask_flat])\n",
    "\n",
    "            total_val_loss += loss.item()\n",
    "            val_batches += 1\n",
    "\n",
    "    avg_val_loss = total_val_loss / max(1, val_batches)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "# Plot training and validation loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
